{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_zxLlltnchi"
   },
   "source": [
    "# Anti-Cancer Drug Activity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4oJOi6qnjMd"
   },
   "source": [
    "## Task 1  Meme competition\n",
    "![xdownload,P20,287,29,P20,281,29.jpeg.pagespeed.ic.lAgGa8zlhA.jpg](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqDAyZKBsqKK"
   },
   "source": [
    "##  Task2 Problem Formulation\n",
    "\n",
    "**What is the input? What is the output?**\n",
    "\n",
    "The input to the model is a graph, which includes nodes representing chemical elements, edges for constructing the structure, and labels for binary classification.\n",
    "\n",
    "The output of the model is a 0/1 binary prediction for againsting cancer cell, 1 for possitive and 0 for negative.\n",
    "\n",
    "**What data mining function is required?**\n",
    "\n",
    "The model serves primarily as a binary classifier. Initially, a dataloader for loading graph data and batching the training data. The processing pipeline embeds the node information and then upsamples the training data to rebalance the data. finally the GNN aggregation approach is employed to provide the binary prediction result.\n",
    "\n",
    "**What could be the challenges?**\n",
    "\n",
    "The challenging part would be resolving the imbalanced data to enhance model performance.\n",
    "\n",
    "**What is the impact?**\n",
    "\n",
    "The trained model can help in predicting if a novel chemical compound can effectively cure cancer, hence reducing considerable costs associated with new medical research.\n",
    "\n",
    "**What is an ideal solution?**\n",
    "\n",
    "the ideal solution would consist of upsampling the positive samples to address data imbalance and embedding node information,  followed by fine tuning to improve the GNN model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54QQ269J-QWA"
   },
   "source": [
    "## Task 3 Understanding the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_zGJrNc_cf8",
    "outputId": "37707f79-5eea-4436-b19d-032625ddba9c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive # connet to google drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "QvGqIz-2-QWC"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "# read in sdf data\n",
    "def read_sdf(file):\n",
    "    with open(file, 'r') as rf: #opening the file\n",
    "        content = rf.read()  #reading the contents in the file\n",
    "    samples = content.split('$$$$')  # Split the read file by $$$$ \n",
    "    # function to config each parse sample\n",
    "    def parse_sample(s): \n",
    "        lines = s.splitlines() #split the text data into lines\n",
    "        links = []\n",
    "        nodes = []\n",
    "        label = 0\n",
    "        #loop over each line\n",
    "        for l in lines:\n",
    "            if l.strip() == '1.0':\n",
    "                label = 1\n",
    "            if l.strip() == '-1.0':\n",
    "                label = 0\n",
    "                #for nodes\n",
    "            if l.startswith('    '):\n",
    "                feature = l.split()\n",
    "                node = feature[3]\n",
    "                nodes.append(node)\n",
    "            elif l.startswith(' '):\n",
    "                lnk = l.split()\n",
    "                # edge: (from, to,) (1-based index)\n",
    "                if int(lnk[0]) - 1 < len(nodes):\n",
    "                    links.append((\n",
    "                        int(lnk[0])-1, \n",
    "                        int(lnk[1])-1, # zero-based index\n",
    "                        # int(lnk[2]) ignore edge weight\n",
    "                    ))\n",
    "        return nodes, np.array(links), label #return nodes, edge and label\n",
    "    \n",
    "    return [parse_sample(s) for s in tqdm(samples) if len(s[0]) > 0]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "cc80fdb8240642c0b53c3ef6897aaf61",
      "63a99574876a46b69741e08f61ab4e95",
      "33d759ba0bd34f4091b465ed83197ae5",
      "287f2db4f4054f22b42decc3422eb18f",
      "1bac1121bea44b9580c350328e7941f9",
      "f8519d6cd6904ded81a1f4bfa61b4ebe",
      "c9bcbcbda05246b28d01b658255bbb08",
      "1e7fb42b5de645c2b7524aaa83ec3988",
      "e1988de71d5d47c2b475701eae61f012",
      "73f1f5bdbe9c4c35a74719bc88293175",
      "02175537a2ae45db81bdb736a63283ca"
     ]
    },
    "id": "yBccx1qR-QWE",
    "outputId": "18b4acc7-a7f2-4fc6-f476-835c35b11b62"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/25024 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc80fdb8240642c0b53c3ef6897aaf61"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # import library\n",
    "\n",
    "training_set = read_sdf('/content/drive/MyDrive/a6/train.sdf') # read in traning data\n",
    "\n",
    "training_set, validation_set = train_test_split(training_set, test_size=0.15,) # split traning and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ea2dadd59e3346c78294818c57c88953",
      "162d6f247f904b9ab90a5bf231650af9",
      "d288ab6801014e149c7de3273cd23b42",
      "ebfbb1c3f5a8412e91d0784ff6026acb",
      "10ec9e7387524bff94f84a34e9e5d47c",
      "fd1abc24809249fb8655a4bfbed20e87",
      "dc4feca36f834df3ab5128f9abbeab91",
      "2c49a41709564a918a007d3944e98307",
      "816cbecd6e3449558aa09c68219f2b1e",
      "10f75132c73748b4a6b87662708a7160",
      "3d3334b0d5cb4b899e6eee184b1a4ae8"
     ]
    },
    "id": "yrCM88LC-QWF",
    "outputId": "c981e48b-9ef5-4bc3-a7b3-e490a2581fa9"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/12326 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea2dadd59e3346c78294818c57c88953"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "testing_set  = read_sdf('/content/drive/MyDrive/a6/test_x.sdf')# read in testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0j-scCs-QWG",
    "outputId": "da300c38-31ca-4bca-db8f-4f3318cefcb2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(['O', 'O', 'O', 'O', 'O', 'N', 'N', 'N', 'N', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'], array([[ 0, 12],\n",
      "       [ 0, 20],\n",
      "       [ 1, 10],\n",
      "       [ 2, 12],\n",
      "       [ 3, 19],\n",
      "       [ 3, 21],\n",
      "       [ 4, 19],\n",
      "       [ 5, 10],\n",
      "       [ 5, 11],\n",
      "       [ 6,  7],\n",
      "       [ 6, 15],\n",
      "       [ 7,  8],\n",
      "       [ 8, 23],\n",
      "       [ 8, 24],\n",
      "       [ 9, 10],\n",
      "       [ 9, 13],\n",
      "       [ 9, 14],\n",
      "       [11, 12],\n",
      "       [11, 16],\n",
      "       [13, 17],\n",
      "       [14, 18],\n",
      "       [15, 17],\n",
      "       [15, 18],\n",
      "       [16, 19],\n",
      "       [20, 22],\n",
      "       [21, 25]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(training_set[1]) #print one sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1-XVs19-QWH"
   },
   "source": [
    "Visualizing/Inspecting a Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "op2Dvnso-QWH"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "!pip install --quiet networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "colors = cm.rainbow(np.linspace(0, 1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "VyeVWFGZ-QWH"
   },
   "outputs": [],
   "source": [
    "# visualize the compound graph \n",
    "def visualize(sample):\n",
    "    G=nx.Graph() #initiating an instance of graph\n",
    "    nodes = sample[0]\n",
    "    edges = sample[1]\n",
    "    \n",
    "    labeldict={}\n",
    "    node_color=[]\n",
    "     #create array for each node color\n",
    "    for i,n in enumerate(nodes):\n",
    "        G.add_node(i)\n",
    "        labeldict[i]=n\n",
    "        node_color.append(colors[hash(n)%len(colors)])\n",
    "\n",
    "    # a list of nodes:\n",
    "    for e in edges:\n",
    "        G.add_edge(e[0], e[1])\n",
    "     #drawing the graph sample with the label\n",
    "    nx.draw(G, labels=labeldict, with_labels = True, node_color = node_color)\n",
    "    plt.show()\n",
    "    \n",
    "    return G #returns graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "bsVlZg2a-QWI",
    "outputId": "69ed4e14-2336-483a-b32f-58af91645005"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3yV5f3/8dd97rMyCUtkhQ0KFBDKDkPAliCIINZZsQqttdavWlC+rdTaiqNVa2u13zqgrp9oWyoOwlQQERlqgqBskY0QRsg4875/f0QjMQlJkDOS83764CG5z3XufMCYd677WoZt2zYiIiIJwhHrAkRERKJJwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIglFwSciIgnFGesCRCSx2LbNMetLfHYRYcK4cNPA0YQkR0qsS5MEoeATkagI2H52BzezPbSBkB085RUDizBNHC3o5O5JE0cLDMOIWZ1S/xm2bduxLkJE6rcDoV2s9y8DIEyoynYmLlIdDRjsvRi34Y1WeZJgFHwiElG7g1vJC7xLmHCN2hs48BrJDE+aiMdIinB1kog0uUVEIuZI+ECtQg/AxsJnF/N+yVvYthXB6iRRaYxPRCJmY2B1laH37qtref3xpezbeoikVA9te7Rm0vRsug7qiI1FoV3AofAeznW2iXLVUt8p+EQkIk5axyiwjlb62vzHlzLvkUXc9JeruWBUV5xuJx8v2cTat/LoOqgjAGGCbAvmKvjkrNMYn4hERK7/Xb4Ibcam/LeYohMlTOk8g1v+fh2DJ/Y57T0cmIxMupwUR4NIlioJRmN8IhIRh8P7KoQewJa1Own4ggy4pFe19zBwcNQ6FInyJIEp+EQkIsqv1fvGyaOFpDdOxXSa1d7DxiJoB852aZLgFHwiEhGGUfm3l7RGqRTkFxIOVT/T08DAoW9TcpbpK0pEIqKqNXhd+rXH5XGy5o3cau9hYOAxks92aZLgFHwiEhFtnedjVjJxPKVBElfdPY6n7pjLmjdy8RcHCAXDfLh4I8/d/Z9ybW1szjFbRatkSRCa1SkiERGyg+QUP1/lFmUrXlnDG39bxt4tB0lK9dLhgkwmTc/mvAEdgNKJLe2cXenhGRzNsiUBKPhEJGJKlzRswab2O7BoKYNEihawi0jEdHMP4MvwXkrswkqXNlTFxMn5ru8r9OLV8V2wfSEc2w4hHzic4G0I7UZCy35gumNd4WmpxyciEVViFbHSNx+fXYRVg56fFbI5L6k3Xd39olCd1MrBXNj0CpTkQzgI3/5hxvSU/rvNUDh/Ejg9US+xJjS5RUQiKsmRwoVJl9HS7IADs9IJL1Day3OGPDxz+6u4dzeKcpVSrW0LYN2TUHgAwgEqhB5A2F/6a9dyePcPECiMdpU1oh6fiERN6WG0W/g8tAm/7cMijBMXGY6mZYfQPvbYY8ybN4/ly5djmtUvcpco2PUObHz5q8CrIcOE1OYw7Ldx9+hTwSciccWyLIYPH86ECRO4/fbbY12OFB+BZf8LVsWdeP65bDuPzN/EjoMnSU92MWFAJg9c24eM1K+CzuEqHffrfmWUiz49BZ+IxJ0dO3YwYMAA3nvvPbp06RLrchLbxrmwcwnY5XfaeeS1Tfzxvxt57n+yGNmjOfvyi7n5Hx9wuMDHqgeycbu+6q07vTD6r3HV69MYn4jEnQ4dOnDvvfcyefJkQqHK1wFKFISD8MXyCqFXUBzgnrm5PD61P6N7t8TldNC2WSqvTh/Gri8LeXHFzm8a28D+9VEtuzoKPhGJSzfddBOpqak8/PDDsS4lcR3eWOnl9zcfxhcIM3FgZrnrqUkuxvRpxZK8/d9cDPvg82WRrLLWFHwiEpccDgezZ8/mkUceYePGyr8BS4T5ToBVcQnKkQIfTdI9OM2KEdK8YRJHCvzfus/xSFV4RhR8IhK3MjMzefDBB7nuuusIBis/5kgiJ+gvxrIqnqLRJN3LkQI/oXDFUDxwrIQm6d9av1fJxJhYUvCJSFy74YYbaN68Offff3+sS6l3LMti3759rFy5kueff57f/e53XHfddQwZMoSWLVvy81/eQYm/4hKGgV2a4nGZzFu9u9z1wpIgOR/tY2SP5uXf4Kz8pI5Y0axOEYl7+/fvp1evXixcuJDevXvHupw6paCggM8//5ydO3eyc+fOcr//4osvyMjIoF27drRv35727duX/b5du3a0TDcwV/6+0h7bH+dt5JH5myrM6jx4vITVD43B8/WsThyQORguuDG6f/DTUPCJSJ3w4osv8tBDD7F+/Xo8ntJHaSetYxRaJwgRwMRFqqMB6Y7E2vUlGAyyZ8+eCqH29e9LSkoqhNrXv2/bti0pKSmn/wTL74ETX1T60rNLtvHnNz4tXceX5OLS/pk8eF1vGqae8qjTdMPQ30J6/BwvpeATkTrBtm0mTJhAt+915ef33MDWQC5F9nEMTGwsDAxsbJKNNDq5etHS2R7TqPv78Nu2zZEjR6rste3fv5/mzZtX2mtr3749TZs2xTCMMy9g31r4+NnSrcjORINMGP77M//8EaDgE5E64/Mvt/Fu8XwyGjXANqve8NrEhYnJYO9YGpiNo1jhmSkpKWHXrl1V9tpcLleVvbbMzEzc7gguDrdC8PZvoPgw2LU8Xsrhgv63wTndIlPbGVLwiUidcDx8hPd8rxO0g9S0A2PiJMs7jobmOZEtrhqWZbF///4qe21Hjx6lTZs2FcbYvv53RkZGTOun5BisuKd00+mahp/phq5XQPuRka3tDCj4RCTu+exilhW/SpDaP25z4WZE0uUkOVIjUNk3Tpw4URZm3w64L774goYNG1b5OLJFixY4HHE+yb7kGKx6qHRNXthXdTuHEzCg1/XQenC0qqsVBZ+IxL1P/WvZHsqr9Dy/d19dy+uPL2Xf1kMkpXpo26M1k6Zn03VQRwAMHLRzdqOHZ9B3qiEYDLJ79+4qe21+v/+0k0iSk5O/0+ePC1ao9Ey+bW9Bwd7SExjsMBgOMIzSj9uNgLYjIKlhrKutkoJPROKaZYdZUPw8ISquJ5v/+FLmPbKIm/5yNReM6orT7eTjJZvYtGob18+6rKydiZMxyZNPO9nl60kklYXa559/zv79+2nRokWVvbYmTZp8t0kkdc3JA3BiFwSLweEuDbom533V44tvCj4RiWv7Qjv4yL+CMOXXkhWdKGFK5xnc8vfrGDyxz2nvYeKipzuLpqHWfP7551X22rxeb4UxtlMnkbhcrkj+USVK4j+aRSShHQ7vrxB6AFvW7iTgCzLgkl7V3iNMkCfn/pm//PSfFSaRDB06lHbt2tGuXTsaNGgQiT+CxBkFn4jEtYBdUun1k0cLSW+ciums2SntPxx3EfcXPRH/k0gk4vQVICJxzVHFz+dpjVIpyC8kHKq4iXJlkr0pCj0BFHwiEueSjVQMKk4a6dKvPS6PkzVv5NboPklG2tkuTeooBZ+IxLXWrk4YlXyrSmmQxFV3j+OpO+ay5o1c/MUBQsEwHy7eyHN3/6dcWxMnmc7O0SpZ4pzG+EQkrqXQgOAxG7OSZWHjb72IjGbp/OuPC/jzjbNJSvXS4YJMJk3PLtcu2Ugjw2wSpYol3mk5g4jErbfffpvp06fTfURHJswcCWbtv12ZOOnpHkKmSz0+KaVHnSISdz799FPGjh3LlClTuOuuu5jz0Mu0cLfDQc1mcH7NgUlTsyWtnZ0iVKnURQo+EYkbBw8e5Gc/+xnDhw9n5MiRfPbZZ/zoRz/C4XDwfc8IzjFbYtZwhMbESWNHc/p6RiXWjipSLQWfiMRcUVER9957L926dSMtLY0tW7Zw++23lx04C+AwTPp7RtPZ1Qsnbkwq30XFxIUTFx1dPRjkza4XZ/LJ2aWvCBGJmXA4zJw5c7jnnnsYOnQo69evp127dlW2NwyDLu4+dHL14kB4F9uDGyi0ThAmhImTFEcaHV09aWG2w2HU7rGoJA5NbhGRqLNtm4ULF3LnnXfSsGFDHn74Yfr16xfrsiRBqMcnIlGVm5vL9OnT2bNnDw899BCXXHKJxuAkqjTGJyJRsWfPHiZPnszo0aOZMGECn3zyCePHj1foSdQp+EQkogoKCvj1r39Nr169aN26NVu3buXmm2/WET8SMwo+EYmIYDDIE088QefOnTlw4AB5eXncd999pKenx7o0SXAa4xORs8q2bebPn89dd91FZmYmixYtomfPnrEuS6SMgk9Ezpo1a9Ywbdo0jh8/zl/+8hd++MMfagxP4o4edYrId7Zz506uvPJKJk6cyE9+8hNyc3MZPXq0Qk/ikoJPRM7Y0aNH+dWvfkXfvn3p1q0bW7du5YYbbsA0tXhc4peCT0Rqze/388gjj3DeeedRVFTEpk2bmDlzJikpKbEuTaRaGuMTkRqzbZtXXnmFX//613Tr1o3ly5fTtWvXWJclUisKPhGpkZUrVzJt2jTC4TCzZ89m+PDhsS5J5Iwo+ETktLZs2cKMGTP46KOPuP/++7nqqqtwODRKInWXvnpFpFKHDx/mlltuISsri4EDB7JlyxauueYahZ7UeerxidQjJVYhX4b3EsCPgYEbL82crfEYSTW+R3FxMY899hiPPvoo11xzDZ999hlNmjSJYNUi0aXgE6njbNvmsLWPbYE88q0DGBhYWAA4cGAHbJqZreno6kkjR7Mq19ZZlsULL7zAzJkz6d+/Px988AEdO3aM5h9FJCp0Hp9IHRayg6z1LSbfOkiY0Gnbmjhpbralt2d4hUNaly5dyvTp00lKSuLhhx9m0KBBkSxbJKYUfCJ1VNgOs9I3nwLrKBbhGr3HxEljR3MGekdjGA42btzI9OnT2bZtGw8++CCXXXaZdluRek+j1CJ11Ef+d2oVegBhQuRbB1hzdBlTpkxhxIgRZGdn8+mnnzJp0iSFniQEjfGJ1EFF1kkOhHdVGnrvvrqW1x9fyr6th0hK9dC2R2smTc+m66DS8bowIXazhXNaNGHr1q1kZGREu3yRmFLwidRBnwc3YlNxlGL+40uZ98gibvrL1VwwqitOt5OPl2xi7Vt5ZcEH4HV7ufHua8hwK/Qk8WiMT6SOsewwC4qfI0Sw3PWiEyVM6TyDW/5+HYMn9qn2Pl4jhdHJ10aqTJG4pTE+kTqmxC6qtLe3Ze1OAr4gAy7pVaP7+O1iwvbpZ4KK1EcKPpE6JmgHMKg4CeXk0ULSG6diOmt2JJCBg6DtP9vlicQ9BZ9IHWMalQdbWqNUCvILCYdqNsvTxsY0NMwviUfBJ1LHeIzkSmdzdunXHpfHyZo3cmt0HwMDJ+6zXZ5I3FPwidQxbsNDY0eLCtdTGiRx1d3jeOqOuax5Ixd/cYBQMMyHizfy3N3/+VZrg1bOjlq3JwlJszpF6qAvw3tZ41tU6TZlK15Zwxt/W8beLQdJSvXS4YJMJk3P5rwBHcraODAZnjSRdEejaJYtEhcUfCJ10OoPVrO5yXs0aJaGw1G7XpuBQQNHE4YnTYxQdSLxTY86ReoQv9/PjBkzmHDpBNK2t8blcNX6Hk7c9PNcFIHqROoGTekSqSM+/PBDJk+eTOfOndmwYQPnnHMOR8MHed+3gBAhqGRt36kMDFx4yEq6hGRHWnSKFolDetQpEueCwSCzZs3iySef5M9//jNXX311uUkphdYJNgU+4FB4D0CFGZ8mTmxsWprt6ebuj9eREtX6ReKNgk8kjn3yySdMnjyZ5s2b8/TTT9OiRcXZnF/z2yXsCn7G/vBOAvZXJ7AbHlqbncl0dcZleKJYuUj8UvCJxKFQKMSf/vQnHn30UR588EFuuOEGLT0QOUs0xicSZzZv3sz1119PWloaH374IZmZmbEuSaRe0axOkTgRDod59NFHycrKYvLkySxevFihJxIB6vGJxIEdO3Zw/fXXYxgGa9asoUOHDtW/SUTOiHp8IjFkWRZPPvkkAwYMYOLEiSxfvlyhJxJh6vGJxMju3bu54YYbKCwsZOXKlZx33nmxLkkkIajHJxJltm3z7LPP0qdPH0aNGsV7772n0BOJIvX4RKJo//79TJ06lQMHDvDOO+/QvXv3WJckknDU4xOJAtu2efHFF+nVqxd9+/ZlzZo1Cj2RGFGPTyTCDh06xM9//nO2bt3KwoUL6d27d6xLEklo6vGJRNC///1vevbsSZcuXfjwww8VeiJxQD0+kQjIz8/nlltu4aOPPuK1115jwIABsS5JRL6iHp/IWfbGG2/Qo0cPmjdvzscff6zQE4kz6vGJnCXHjx/ntttuY+XKlbz88ssMHTo01iWJSCXU4xM5CxYtWkSPHj1ISUkhLy9PoScSx9TjE/kOTp48ybRp01i4cCGzZ89m1KhRsS5JRKqhHp/IGVq+fDk9e/YkFAqxYcMGhZ5IHaEenySkICE2sZ+17OQEJYQJ48SkEakMpAOdORezip8Li4uLmTFjBvPmzeMf//gHF198cZSrF5HvQsEnCSWMxdt8ysfsBiBI+JTXQhzgOG+Si4HBQDoyiI4YfHPy+fvvv8/1119Pv3792LBhA40aNYr6n0FEvhvDtm071kWIRIOfEC+zmkMUEMKqtr0Lk/Y0ZSJ9CPgC/Pa3v+WFF17gySefZMKECVGoWEQiQcEnCcHC4kVWs5/jhGsQel9zYtI0380jQ/+H888/n7///e80bdo0gpWKSKQp+CQhrOdz3uazco82H247hWCxn199/jTuFG9pu2cWk/vicqYsv7+sXaDIR+Y6mxuGXY5hGBXuLSJ1i2Z1Sr1nY7OaHeVC72tW2OL9v7x+2ve7U7y4h7dU6InUEwo+qff2cJQSApW+ljV9Aqsefo2S44Wnvcd+jnOc4kiUJyJRpuCTei+PPZX29gBafr8j7YZ3572HXzvtPSxsPmV/JMoTkShT8Em9V0DJaV8f+ftr+ODxNyk6fKLKNhZ2tfcRkbpBwSf1XnWzOJt1b0OXsX1598F/n7ZdqIpeo4jULQo+qfeScFfbZuS9V7H+6cUU7Muvsk0K3rNZlojEiIJP6r32NMWFedo2jTu2oPsVQ1j91zcrfd2NSSbapUWkPlDwSb3XnVbYVL9c9cLfXkGwyFfpay6ctEcL10XqAy1gl4TwFnnksbsG8VeREwdD6cJAOp71ukQk+tTjk4QwmE7YgZpvVXYqLy4uoM1ZrkhEYkXBJ/WeZVncf+fvyLnuCZxWzXdfMQAPTq5lEF5ckStQRKJKxxJJvebz+Zg8eTL79u1j/vz5BBwmL/MBIawqF7VD6WSWJNxcyyAySI5ixSISaRrjk3orPz+fSy+9lObNm/P888/j9ZYuRwgR5jMOsJrtHKMYEwOb0h5eGItzacAAOtKJc3DooYhIvaPgk3pp586djBkzhnHjxvHQQw/hcFQeYEc4yTGKCRLCjZMmpKmHJ1LPKfik3lm3bh3jx4/nN7/5Db/4xS9iXY6IxBmN8Um98vrrr3PjjTfy7LPPcskll8S6HBGJQwo+qTeeeOIJZs2axYIFC+jbt2+syxGROKXgkzrPsizuvPNO3nzzTVatWkW7du1iXZKIxDEFn9RpPp+P6667joMHD/L+++/TqJH20xSR09Ncbamz8vPzGTVqFKZpsnjxYoWeiNSIgk/qpB07djBw4ECysrJ46aWXytboiYhUR8Endc6aNWvIysrijjvu4MEHH6xyjZ6ISGU0xid1ymuvvcbUqVOZM2cOY8eOjXU5IlIHKfikzvjrX//KQw89RE5ODt///vdjXY6I1FEKPol7lmUxbdo0Fi5cyKpVq2jbtm2sSxKROkzBJ3GtpKSEa6+9lvz8fFatWkXDhg1jXZKI1HGaFSBx68iRI4wcORKv18uiRYsUeiJyVij4JC5t376dQYMGceGFF/LCCy/g8XhiXZKI1BMKPok7q1evZsiQIUybNo1Zs2ZpuYKInFUa45O4Mm/ePG666Saee+45srOzY12OiNRDCj6JG4899hgPP/wwCxcupHfv3rEuR0TqqfgPPtuGY9th+0I4thPCfnA4wZsB7UZBy/7g1PhPXRYOh/nVr37FkiVLWLVqFW3atIl1SSJSj8X3Cez71sGn/wL/CQgHgG+Van4VeG2GQtfLwXRHvUT5boqLi7n22ms5fvw48+bNIyMjI9YliUg9F7/Bt/k12L7gq8CrhsMFqefC4LvAnRr52qRSBdYxdgQ3cDC8m5AdwMDAZbhpZXainasbyY7y/20OHz7MuHHj6NSpE88++yxut35wEZHIi8/g274QNs+rWeh9zTAhrQUMnameX5QdC39JbmAlJ61j2FjY3+qZOzABaOw4l16eIaQ4GrB161bGjBnDVVddxe9//3sMw4hF6SKSgOIv+AoPwjszwQpWeOmfy7bzyPxN7Dh4kvRkFxMGZPLAtX3ISP0q6BwuaP8D6HZ5lItOXAdCu1jvX0aYUA1aGzhxkba1NVdkX8usWbO48cYbI16jiMip4i/48p6HL1aAHS53+ZHXNvHH/27kuf/JYmSP5uzLL+bmf3zA4QIfqx7Ixu0q7VXgTILsx0snwEhEHQnv533fAizC1Tc+RclJHxlbOpA9TKcriEj0xdfK4JAf9qyqEHoFxQHumZvL41P7M7p3S1xOB22bpfLq9GHs+rKQF1fs/KaxbcOBD6NceOKx7DBrfIsrDb13X13LtCH3c1Wz/+GGDnfy+wmP8+n728teT0r14unri2a5IiJl4iv4vtwAlYz1vL/5ML5AmIkDM8tdT01yMaZPK5bk7f/mYtgHn78d6UoT3oHwLiysCtfnP76UZ+/8F5dNy2bOzj/y1OYHyJ46jLVv5X3TyIAi+wQnwvlRrFhEpFR8PQ8sOQ5WxR7EkQIfTdI9OM2KOd28YRIf7ij/DTRUeISTx47hdDpxuVy4XC5M04xY2YloazCXMOXHYYtOlDD3vje45e/XMXD8BWXX+47pQd8xPcq1tQizPbSBPuaFUalXRORr8RV8VhDsir2IJulejhT4CYWtCuF34FgJTdLLL2A/uH8v32vfnmAwWPYLKBeEp/6+th/H03tN04z6jEifVcRJ61iF61vW7iTgCzLgkl7V3sPGZl9oB73dwzWjU0SiKr6Cz5UMDhPC5Xt9A7s0xeMymbd6Nz/Kalt2vbAkSM5H+7j/2gvKtW/VtiPHjr1S7pplWeWCMBQKVfr72n5c1Ws+n++M31ubjy3LinoAp5zrptWEVEx3+cA6ebSQ9MapmM6a9a5tLMKEcOKqxReJiMh3E1/B17BDpZcbpLi554qe/PLpNaQnu8rN6mzVOJkfDz/lfYYJTc6rcA+Hw4HH46l3x9t8HeiRCO/KPvb5fASTTVqGk4HyAZfWKJWC/ELCoXCNws/AqHScUEQkkuIr+Bq0hpRzoGBvhZfunNidxmkepv1zfek6viQXl/bP5KU7huBxnfJN1nBA+4uiWHRsxSLQT1rHWV4yr8IYX5d+7XF5nKx5I5dBE/pUex8LC5d6eyISZfEVfACdLobcf5ZuRv0tN17UiRsv6nT69zfILN2+TCImxUjDoOK4XEqDJK66exxP3TEX02nSa2RXTJdJ3jufsfHdLUy+77Jy7dOMhhhGfE0sFpH6L/4WsIeD8PZvoORIpRNdTsvhgkHToXHnyNQmZTb517AjtKHSR5UrXlnDG39bxt4tB0lK9dLhgkwmTc/mvAHfPJI2cdHLk0Vrp/5biUh0xV/wAZQcheX3QLCo5uFnuqHHjyFzSGRrEwCKrUKWlsyt9a4tX3PiIjv5Okwj/h46iEj9Fp/PmZIawfDfQ3JTcHpP39bhAocbek9V6EVRsiOVVs6OmNR+faSJky6uPgo9EYmJ+Ozxfc0Kw6Fc2LYATuwuXepgW4ADDL7alPoiaDscPOkxLjbxWLbF+763OGp9iVWjTaoh6Auxc9V+bh99j44hEpGYiO/gO1XhwdLwCxaXPtb0NoQmXUpncUrMWHaYj/zLv9rCLFzhSKKvGTgwMGjv6M5dk/5Aeno6L774onbUEZGoqzvBJ3HtRDif7aEN7Avt+Or8vfJfVm2c59He1Z0URzo+n49x48bRsmVLZs+ejcOhH15EJHoUfHJWBW0/R8OHCODHwMBteGnsOLfCeF5RURHZ2dmcf/75/N///Z+2LRORqFHwScycPHmSH/zgB/Tr14/HHntM4SciUaFnTBIzaWlp5OTk8N577zFjxgz0M5iIRIOCT2IqIyODxYsXk5OTw7333hvrckQkAWghlcRc48aNWbJkCcOHD8fr9TJjxoxYlyQi9ZiCT+JCs2bNWLZsGcOGDcPr9XLbbbfFuiQRqacUfBI3WrRoUS78brrppliXJCL1kIJP4kpmZiZLly5l+PDheDwefvKTn8S6JBGpZxR8Enc6dOjA0qVLufDCC/F4PFx99dWxLklE6hEFn8SlLl26sHjxYkaNGoXH4+Gyyy6r/k0iIjWg4JO41b17d3Jychg9ejQej4exY8fGuiQRqQe0c4vEvbVr1zJ27FheeuklLrrooliXIyJ1nBawS9zr168f8+bN45prrmHFihWxLkdE6jgFn9QJWVlZzJ07l0mTJvH+++/HuhwRqcMUfFJnjBgxghdeeIFLL72U9evXx7ocEamjFHxSp4wePZqnn36asWPHkpeXF+tyRKQO0qxOqXPGjx9PIBBg9OjRLFu2jK5du8a6JBGpQxR8Uiddfvnl+P1+fvCDH/DOO+/QqVOnWJckInWEgk/qrGuvvRafz8eoUaNYvnw57dq1i3VJIlIHKPikTpsyZQp+v5+RI0fy7rvv0qpVq1iXJCJxTsEndd4vfvELfD4fI0aMYMWKFTRv3jzWJYlIHFPwSb3wq1/9ipKSkrLHnk2bNi3foGAPHMwD34nSj70NoFlPaNA6+sWKSExpyzKpV+6++27efPNN3n77bRplNID962DbW1B0CKwQ2FZpQ8MBDieknAOdLoYWfUs/FpF6T8En9Ypt20yfPp0PP3iPJX/Ixlm0H8L+07/J9EB6axh4B7iSo1OoiMSMgk/qHTvoY/8rP6dJUhiPsy/ujrgAABKjSURBVIZ7NDickNIMhv4WnJ7IFigiMaXgk/pn7d+wD+VhWMFyl/+5bDuPzN/EjoMnSU92MWFAJg9c24eMVHdpA4cLmn0P+t0ag6JFJFq0ZZnUL8X5UEnoPfLaJu56/kP+dP33OfH/ruaDhy7miy+LuOh3iwkEw6WNrCAc+gSKj8SgcBGJFgWf1C+fLwXKP8QoKA5wz9xcHp/an9G9W+JyOmjbLJVXpw9j15eFvLhi5zeNbQt2Lo1uzSISVQo+qT9sC3YtL529eYr3Nx/GFwgzcWBmueupSS7G9GnFkrz9p9wjDF8sBysc+XpFJCYUfFJ/BAorhB7AkQIfTdI9OM2KX+7NGyZxpOBbsz6tMARORqpKEYkxBZ/UH8GS0vV539Ik3cuRAj+hsFXhtQPHSmiS/q1ZnA4TQr5IVSkiMabgk/rD6flmgfopBnZpisdlMm/17nLXC0uC5Hy0j5E9vrXFmW2Vru0TkXpJW1VI/eFOrfRygxQ391zRk18+vYb0ZBcjezRnX34xN//jA1o1TubHwzuUf4NtV3kvEan7FHxSfzic0GoA7FlVoed358TuNE7zMO2f60vX8SW5uLR/Ji/dMQSPy/ymoeGAlv3AdEW5eBGJFi1gl/qlYA+8+wcIB87s/aYbhszU5tUi9ZjG+KR+SW8N6a3AMKtv+y0hC6zUFgo9kXpOwSf1T79bwZUCGDV+i41Bod/iikdWcezYscjVJiIxp+CT+sebAUNngrchGDUYxjacGN4M0sY8ROtO32PAgAFs27Yt8nWKSExojE/qr0AhbFsAu94pnezy7eOJTC8YBrQZDp0vLpvJ+dRTTzFz5kxeeeUVhg8fHvWyRSSyFHxS/1kh2L8e9q0Bf0HpNU86tOwPzftUOoNz2bJlXH311cyaNYspU6ZEuWARiSQFn0gVtmzZwtixY7n00kt58MEHMc3aT5gRkfij4BM5jfz8fCZNmkR6ejovvfQSqala2C5S12lyi8hpNG7cmEWLFtG0aVOysrLYs2dPrEsSke9IwSdSDbfbzdNPP82Pf/xjBgwYwNq1a2Ndkoh8B3rUKVILr7/+OjfeeCNPPPEEP/rRj2JdjoicAQWfSC3l5uZyySWXMHXqVO6++24Mo+YL5UUk9hR8ImfgwIEDjB8/nk6dOvHss8/i9XpjXZKI1JDG+ETOQPPmzVmxYgWhUIgRI0Zw6NChWJckIjWk4BM5Q0lJSbz88stcdNFF9O/fn08++STWJYlIDehRp8hZ8NJLL3Hbbbfx3HPPMWbMmFiXIyKnoeATOUtWr17NZZddxl133cWtt96qSS8icUrBJ3IW7dq1i3HjxpGVlcVf//pXXC6d5C4SbxR8ImdZQUEBV155JYFAgH/96180bNgw1iWJyCk0uUXkLEtPT+f111+ne/fuDBw4kO3bt8e6JBE5hYJPJAKcTiePPfYYt912G1lZWaxYsSLWJYnIV/SoUyTCli5dyjXXXMMDDzzADTfcEOtyRBKegk8kCjZv3szYsWOZOHEiDzzwgM72E4khBZ9IlOTn5zNx4kQaNmzIiy++qLP9RGJEY3wiUdK4cWOWLFlC48aNGTJkCHv37o11SSIJScEnEkVut5tnnnmGq6++mgEDBrBu3boq29q2zbHwl+wP7WR3cCsHQrsosk5GsVqR+kmPOkVi5LXXXmPq1Kk8+eSTXH755WXXg3aA3cGtbA/lEbB9GBjY2BgYWFg0dDSlk6sXzczWGIZ+dhWpLQWfSAx9/PHHjB8/np/+9Kf85je/4bC1jzW+RQCECVX5PhMXSUYyg73jSHKkRKtckXpBwScSY/v372f8+PEM/9FAhtzcA4twjd5nYODCw4VJl5Hk0EQZkZpS8InEgYNFe3nP/zpOT+2WORgYJBmpjEz6EabhjFB1UhNhO4TPLiZEECduvEYypqFlK/FI/6eIxIGdjrwqQ+/dV9fy+uNL2bf1EEmpHtr2aM2k6dl0HdQRGxu/XcK+0A4yXV2iXLUAHA8fYXswj/3hzzG++semtD+R6exMe9f3SHNkxLhKOZWCTyTGSqwijlgHKn1t/uNLmffIIm76y9VcMKorTreTj5dsYu1beXQd1BEoHQvcGsxV8EWZzyriA/8iTlrHsAiXhd2pdoU+44vQFho7zqWf9yJchicGlcq36VGnSIx96l/L9lAeFla560UnSpjSeQa3/P06Bk/sc9p7mDjJ8l5CQ7NpJEuVrxRZBawo+S9BAtjf+u9WGQcOvEYKw5Im4DGSolChnI7mQovE2JfW3gqhB7Bl7U4CviADLulV7T1sbI5ahyJRnnxLwPbznu91AvhrFHoAFhYldhGrSt4kbFc9W1eiQ486RWIsaAcqvX7yaCHpjVMxndVPkAjbYdbnriVvz1aSk5Or/JWUlKR9Qr+j7cE8/HYJVPJo8/TjsRZFdgF7Qtto6zo/+oVLGQWfSIw5qnjwktYolYL8QsKhcLXhZ1s2Wzdv5eP/bqW4uPi0v9xu92nD8Wz88ng8GIYRib+umLLsMJ8HN1XaQ6/peOy2YC5tnOfVy7+fukJjfCIxZFkWiw6/gj+1oMJrRSdKuLHTXdz6j8kMmlD9GF9Pd1a1E1xs28bv91cbjqf+KioqqlX74uJigsFgxMM1OTkZpzO6P7vvC+3kY/9yQgTLXa/teOxg71gamc0iWaqchnp8IlF29OhRFi9eTE5ODgsXLmTIZX258g9jcHrL9+pSGiRx1d3jeOqOuZhOk14ju2K6TPLe+YyN725h8n2XlbW1sTnX2bbaz20YBl6vF6/XS6NGjc72H61MKBSipKSk1oGZn59fq/amaVYIw5SUlLMarl6vt6x3tj+0s0LoQe3GY8OEOBj6QsEXQwo+kQizLIuPP/6YnJwcFixYwMaNGxk2bBhjxozh3nvvJbNNJjnFzxPEX+G942+9iIxm6fzrjwv4842zSUr10uGCTCZNz/6mkQ0tnO1wx9FUeafTSVpaGmlpaRH7HLZtEwgEah2uR48eZe/evTVu7/f7SUpKIjk5melzp9B5QNsKtdRmPBbAR/FZ/tuQ2lDwiUTAsWPHyvXqMjIyyoJuyJAheL3ecu3bu7qzLZhb6XZlw67oz7Ar+lf5ufy+AGtzNnLB1cMTauKKYRh4PB48Hg8NGzaM2OexLKus95rnXE4hRyu0qc14LIBG92JLyxlEzgLLsvjoo4+YNWsWgwcPpk2bNrzwwgv069eP1atXs3nzZh599FEuuuiiCqEH0NnVizQjA6OW/0uaOGkZ6sTcp//D0KFD2bJly9n6I8lXHA4HKSkpNG3alIykyh8Pd+nXHpfHyZo3cmt0Ty/aWDyWFHwiZ+jYsWO8+uqr/OQnP6FFixZcddVVHD58mHvuuYcvv/ySN998k5tvvpl27dpVey/TcDI4aSxpRgYOatZrM3GS6ezC4GY/ZPny5Vx55ZUMHjyYP/3pT4TDNdvoWmqnpbMDTlwVrp86HrvmjVz8xQFCwTAfLt7Ic3f/p1xbEyfNazAeK5GjWZ0iNWTbNrm5uWVjdRs2bGDIkCGMGTOG7Oxs2rdv/50/R8gOsjHwAXtCWwCj0qOJnLgwDSfnu/pWWA+2c+dOpkyZQlFREXPmzKFr167fuSb5hm1b5BS/QABfpa+veGUNb/xtGXu3HCw3HnvegA5lbdKMDEYmXxGtkqUSCj6R0zh+/DhLliwhJyeHnJwc0tLSyoJu6NChJCVFZvupkB1kT2gbO4Mb8dlFWFiYOGngaEwnV0+amq2qXAdmWRZPPfUUM2fO5Pbbb+fOO++M+rT/+mxL4EO2BD+u8fFRp6rpshOJLAWfyCls2yYvL68s6HJzc8nKyioLuw4dOlR/kzjxxRdfMHXqVPLz85kzZw49evSIdUn1QtAOsLzkPxTbJyvdmLoqDhw0cDRhiPcSHDquKKYUfJLwTpw4Ua5Xl5KSQnZ2NmPGjGHYsGER69VFg23bzJ49mxkzZnDLLbfwv//7v7jd7liXVeeVWEW86/svPrukhptUm6QY6QxJGh9Xy04SlYJPYiJkBwnYPizCOA03HpKitoWTbdts2LChLOg++ugjsrKyyM7OJjs7m06dOkWljmjau3cvP/vZz9i7dy9z5syhd+/esS6pzgvYPtb5lpJvHcTGrjQAv56odK7Zht6e4TiNihNjJPoUfBI1tm2Tbx1kezCPQ+E9OHBgYGBh4cRFB9f3aOs6PyLHtpw4cYKlS5eWravzer3lenXJycln/XPGG9u2eeGFF5g2bRo//elPmTlzJh6Peh/fVaF1gh3BT9gd2oKNhYHjq69pJ+2c3Wjn6kaSQ8sX4omCT6KiwDrKB75F+O3iSmcqwjc/HbdxnkcP9yAM48xX29i2zSeffFKuVzdo0KCysKuPvbqaOnDgAD//+c/Zvn07c+bMoW/fvrEuqV6wbZsgAUJ2AKfhxoVbG1HHKQWfRNzR8EHe9y2odI/Dypg4aeQ4l4He0bWaBFBQUFDWq8vJycHj8ZQ9vrzwwgsToldXU7ZtM3fuXG677Tauv/567r333koX1ovURwo+iahC6wTLS/5T49D7momT5mZbvu8dWWUb27bZtGkTCxYsICcnh/Xr15f16rKzs+ncubN+4q7GoUOHuOWWW/jkk0+YPXs2gwYNinVJIhGn4JOIWu3L4VB4d6Wvne7QTigNvyzvOBqa55S95+TJkyxbtqws7JxOZ9lSgwsvvJCUFI2lnIl///vf/PKXv+Sqq67ivvvuU+9Y6jUFn0SMzypiccnLlS70rerQzk2rtnH9rK+P2zFoYbYjZUeLsqBbt24dAwcOLOvVdenSRb26s+TIkSPceuutrFu3jmeffZahQ4dW+x7btjlhHaHELiqdoYubBo7GeB0KTolfCj6JmM8C69gWzKsQfLU5tDPoC3LPiCcYPvjCsl5dampqJMtOePPnz+fmm29m4sSJPPDAA5X+fQdtP7uDW9keyiNgB045bcDAIkxTsyWdXD1p7GiuH0wk7ij4JGKWFs+l0D5R4fpHSzYxa9ITvJr/eLVHuDgsk97eC2nlqjs7ptQHR48e5fbbb2flypU888wzjBgxouy1g6HdrPMvAahyhi6UPqpOczRkkPdiLdqWuKLTGSRignag0uu1ObTTcFDpAa0SWY0aNeK5557j8ccfZ/Lkydx0000UFBSwN7iddf7FhAmdNvSgNBRPWPksL/k3AbvyTZ1FYkHBJ1F36qGdEt8uvvhiNm7cSCgUYty1o1lf8jbhWmzObGNRYhezyvcmtl391l4i0aAt2yViXIYHv11S4fqph3YOmnD6MT4Dhx6TxViDBg145plneP3AP7HMynvfp5uha2NRaBVwKLyHc51toly9SEUKPomYls4ObAvmVpjccuqhnabTpNfIrpguk7x3PmPju1uYfN9lZW0tLJqaLaNdunxLoXUc0it/tFnVDN21b+WVLU0JE2RbMFfBJ3FBk1skYk63nAFqcminQUuzPX29o6JXtFQqz/8eu0KfVjiGpzYzdB2YjEy6nBRHg0iWKlIt9fgkYryOFJqaLTkU3gOVnFs27Ir+DLuif5XvNzHp6OoZwQqlpr4M76307Lkta3cS8AUZcEmvau9hYHDUOqTgk5jT5BaJqO+5B+Gk9kexmDhpYbajodk0AlVJbYX47jN0bWwCVcz0FYkmBZ9EVKqjAYO8Y2oVfiZOGjuac4FneOQKk1oxqvhWUZsZugalp5CLxJq+CiXiGpnNGJY0gRQjHfM0T9cdmDgwaeM876uTGfTlGS+qOiPx1Bm61TFwROSsRZHaMn/3u9/9LtZFSP3nMZJo7+xOE7MFQdtPkV2AiVl2GK0bL51dF/B97whaONtrm6s4dCS8v8Ip426vC5fHyUv3zqd5h3No2roRAB8v+5Qlc1bSa0TXsrYGDnp5htTqqCmRSNCsTomJsB0iYPsJE8JluHHjVdjFsZAdJKf4+Sp3a6luhq6Bg7bO8+npyYpm2SKVUvCJSI3k+t/li9CWCr2+mnBgMiJpEqmOjAhUJlI7GkQRkRrp5u5PkpGCQe165iZOznP1UehJ3FDwiUiNuAwPQ7yXkGSk1nh2pomT9s5udHJVv85PJFr0qFNEaiVg+8nzr+RAeBdApTvzmLhwGi66ufqR6eoS5QpFTk/BJyJnJGD72BXczK7Qp/jtEmwsTFxkOJrQydWTpmYrTViSuKTgExGRhKIxPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSgKPhERSSj/H/Y51j5+gRf5AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7fa41a2619d0>"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "plt.clf() #plot one sample\n",
    "visualize(training_set[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4sGO3vg-QWJ"
   },
   "source": [
    "Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Ty0mBTXK-QWJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_vocab = 500 #vocabulary size\n",
    "max_len = 100 # maximum length of the tokenizer\n",
    "\n",
    "# build vocabulary from training set\n",
    "all_nodes = [s[0] for s in training_set]\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(all_nodes) #training tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "NoRHeLy2-QWJ"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "random.seed(0) # set random seed\n",
    "\n",
    "# prepare single batch of the data\n",
    "def prepare_single_batch(samples):\n",
    "    sample_nodes = [s[0] for s in samples] # Nodes characters \n",
    "    sample_nodes = tokenizer.texts_to_sequences(sample_nodes) # tokenizing the sample nodes\n",
    "    sample_nodes = pad_sequences(sample_nodes, padding='post') # pad_sequences for each sample\n",
    "    max_nodes_len = np.shape(sample_nodes)[1] #get the shape\n",
    "    edges = [s[1]+i*max_nodes_len for i,s in enumerate(samples)]# indexing current nodes\n",
    "    edges = [e for e in edges if len(e) > 0] #check the element is non-zero\n",
    "    node_to_graph = [[i]*max_nodes_len for i in range(len(samples))] # create segmented_ids\n",
    "    \n",
    "    all_nodes = np.reshape(sample_nodes, -1) #reshape the nodes of all samples \n",
    "    all_edges = np.concatenate(edges)  #concatenate edges of all samples\n",
    "\n",
    "    node_to_graph = np.reshape(node_to_graph, -1)\n",
    "    return { #returns a dictionary of features\n",
    "        'data': all_nodes,\n",
    "        'edges': all_edges,\n",
    "        'node2grah': node_to_graph,\n",
    "    }, np.array([s[2] for s in samples])\n",
    "\n",
    "\n",
    "#generating batch\n",
    "def gen_batch(dataset, batch_size=16, repeat=False, shuffle=True):\n",
    "    while True: # loop over\n",
    "        dataset = list(dataset) #data in the array\n",
    "        if shuffle: # if shuffle is True\n",
    "            random.shuffle(dataset) #randomly shuffle\n",
    "        l = len(dataset) #length of dataset\n",
    "        for ndx in range(0, l, batch_size): #loop over for creating batches \n",
    "            batch_samples = dataset[ndx:min(ndx + batch_size, l)] #creating batch samples\n",
    "            yield prepare_single_batch(batch_samples) #call prepare_single_batch \n",
    "        if not repeat: #breaking loop \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZL7kI4G-QWK",
    "outputId": "af89e0ef-cdc7-499b-b659-a7ee4a95dfbe"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data\n",
      "[5 8 2 2 2 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 4\n",
      " 4 2 2 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 8 2 2 2\n",
      " 2 2 2 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 2 2 3\n",
      " 3 3 3 3 3 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "edges\n",
      "[[  0   5]\n",
      " [  1   2]\n",
      " [  1   3]\n",
      " [  1   8]\n",
      " [  1   9]\n",
      " [  2  11]\n",
      " [  4   6]\n",
      " [  5   6]\n",
      " [  5   7]\n",
      " [  6   8]\n",
      " [  7   9]\n",
      " [  7  10]\n",
      " [ 11  12]\n",
      " [ 35  51]\n",
      " [ 36  38]\n",
      " [ 36  39]\n",
      " [ 36  40]\n",
      " [ 36  45]\n",
      " [ 37  47]\n",
      " [ 40  46]\n",
      " [ 41  46]\n",
      " [ 41  53]\n",
      " [ 41  54]\n",
      " [ 42  55]\n",
      " [ 42  61]\n",
      " [ 42  62]\n",
      " [ 43  58]\n",
      " [ 44  59]\n",
      " [ 45  47]\n",
      " [ 45  48]\n",
      " [ 46  50]\n",
      " [ 47  52]\n",
      " [ 48  49]\n",
      " [ 49  51]\n",
      " [ 49  60]\n",
      " [ 50  58]\n",
      " [ 50  59]\n",
      " [ 51  52]\n",
      " [ 53  56]\n",
      " [ 54  57]\n",
      " [ 55  56]\n",
      " [ 55  57]\n",
      " [ 70  71]\n",
      " [ 70  72]\n",
      " [ 70  73]\n",
      " [ 70  74]\n",
      " [ 71  80]\n",
      " [ 72  81]\n",
      " [ 75  79]\n",
      " [ 75  85]\n",
      " [ 76  84]\n",
      " [ 77  83]\n",
      " [ 77  86]\n",
      " [ 77  87]\n",
      " [ 77  88]\n",
      " [ 78  82]\n",
      " [ 78  84]\n",
      " [ 79  80]\n",
      " [ 79  82]\n",
      " [ 81  83]\n",
      " [ 84  89]\n",
      " [ 85  91]\n",
      " [ 89  90]\n",
      " [ 90  92]\n",
      " [ 92  93]\n",
      " [ 93  94]\n",
      " [ 94  95]\n",
      " [ 95  96]\n",
      " [ 96  97]\n",
      " [ 97  98]\n",
      " [ 98  99]\n",
      " [ 99 100]\n",
      " [100 101]\n",
      " [101 102]\n",
      " [102 103]\n",
      " [103 104]\n",
      " [105 116]\n",
      " [106 119]\n",
      " [107 120]\n",
      " [108 117]\n",
      " [109 118]\n",
      " [110 112]\n",
      " [110 116]\n",
      " [110 117]\n",
      " [111 113]\n",
      " [111 116]\n",
      " [111 118]\n",
      " [112 119]\n",
      " [113 120]\n",
      " [114 117]\n",
      " [114 119]\n",
      " [115 118]\n",
      " [115 120]]\n",
      "node2grah\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "label [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# showing one batch:\n",
    "for train_batch in gen_batch(training_set, batch_size=4):\n",
    "    for k,v in train_batch[0].items():\n",
    "        print(k) #k: labels\n",
    "        print(v) #v: values\n",
    "        pass\n",
    "    print('label', train_batch[1])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "MyXMySGK-QWL"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet tf2_gnn #installing tf2_gnn\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "\n",
    "from tf2_gnn.layers.gnn import GNN, GNNInput #import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yx6gX_A-QWL",
    "outputId": "829e451a-94da-4217-bbd0-4781a9959978"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), name='gnn_9/StatefulPartitionedCall:0', description=\"created by layer 'gnn_9'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), name='tf.math.segment_mean_9/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_9'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_9/Sigmoid:0', description=\"created by layer 'dense_9'\")\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_9 (TFOpLamb  ()                  0           ['input_30[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 20)           10000       ['input_28[0][0]']               \n",
      "                                                                                                  \n",
      " input_29 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  ()                  0           ['tf.math.reduce_max_9[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_9 (GNN)                    (None, 32)           22464       ['embedding_9[0][0]',            \n",
      "                                                                  'input_29[0][0]',               \n",
      "                                                                  'input_30[0][0]',               \n",
      "                                                                  'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_9 (TFOpLa  (None, 32)          0           ['gnn_9[0][0]',                  \n",
      " mbda)                                                            'input_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            33          ['tf.math.segment_mean_9[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,497\n",
      "Trainable params: 32,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import segment_mean # the mean of elements in segments of a tensor.\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "params[\"hidden_dim\"] = 32 #change default hyperparameters\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "dXhxyxOV-QWM"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gB_7XJWx-QWM",
    "outputId": "3bf72533-e5b1-48ab-da56-d000d6e2200f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_3_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_3_grad/Identity:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/cond_3_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_1_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_1_grad/Identity:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/cond_1_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgcn/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1330/1330 [==============================] - 22s 15ms/step - loss: 0.2377 - auc: 0.4554 - val_loss: 0.1916 - val_auc: 0.6511\n",
      "Epoch 2/5\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 0.1967 - auc: 0.6154 - val_loss: 0.1878 - val_auc: 0.6805\n",
      "Epoch 3/5\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 0.1941 - auc: 0.6421 - val_loss: 0.1790 - val_auc: 0.7109\n",
      "Epoch 4/5\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1902 - auc: 0.6661 - val_loss: 0.1791 - val_auc: 0.7049\n",
      "Epoch 5/5\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1887 - auc: 0.6661 - val_loss: 0.1836 - val_auc: 0.7191\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4fadf91f0>"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "# fit in the data\n",
    "model.fit(\n",
    "    gen_batch(\n",
    "        training_set, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=5,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EB4ZCQcg-QWM",
    "outputId": "4fb2457e-1950-4f0f-db2e-32dc09e904f8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "771/771 [==============================] - 3s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict( # generate prediction using trained model\n",
    "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
    ") \n",
    "y_pred = np.reshape(y_pred, -1) #reshap the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axYU-oK5-QWN",
    "outputId": "90247631-728a-4c0d-e15b-a80008e0a901"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12326"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "len(y_pred)#print lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "1uN1UWD--QWN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # prepare csv file for submision\n",
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "7LCNwp-J-QWO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4 Model Tuning"
   ],
   "metadata": {
    "id": "lGki0EKVwoDw"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i49lxG6TQ4gs"
   },
   "source": [
    "### Tiral 1-2 tuning the templet model\n",
    "\n",
    "Observing the previous training results, it is clear that the model requires improvement; thus, I want to conduct two separate trials with rising and decreasing model complexity to evaluate its performance.In adition, I plan to increase the number of traning epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4f09HdEQ8FN",
    "outputId": "cd10cfcb-e412-414e-b7ce-de968f371503"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='gnn_10/StatefulPartitionedCall:0', description=\"created by layer 'gnn_10'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='tf.math.segment_mean_10/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_10'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_10/Sigmoid:0', description=\"created by layer 'dense_10'\")\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_31 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_10 (TFOpLam  ()                  0           ['input_33[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_10 (Embedding)       (None, 20)           10000       ['input_31[0][0]']               \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  ()                  0           ['tf.math.reduce_max_10[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_10 (GNN)                   (None, 64)           120704      ['embedding_10[0][0]',           \n",
      "                                                                  'input_32[0][0]',               \n",
      "                                                                  'input_33[0][0]',               \n",
      "                                                                  'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_10 (TFOpL  (None, 64)          0           ['gnn_10[0][0]',                 \n",
      " ambda)                                                           'input_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1)            65          ['tf.math.segment_mean_10[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 130,769\n",
      "Trainable params: 130,769\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters\n",
    "params[\"hidden_dim\"] = 64 \n",
    "params[\"num_layers\"] = 16\n",
    "params[\"dense_every_num_layers\"] = 8\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 12\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model1 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model1.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gw0XJ2KwRQpx",
    "outputId": "81229a08-9f64-42ea-f0a8-cfa461241cd9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_15_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_15_grad/gradients/grad_ys_0_values:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_15_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_15_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_15_grad/Identity:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_15_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_14/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_14/embedding_lookup_grad/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_14/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_13_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_13_grad/gradients/grad_ys_0_values:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_13_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_13_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_13_grad/Identity:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_13_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_12/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_12/embedding_lookup_grad/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_12/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_11_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_11_grad/gradients/grad_ys_0_values:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_11_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_11_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_11_grad/Identity:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_11_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_10/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_10/embedding_lookup_grad/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_10/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_9_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_9_grad/gradients/grad_ys_0_values:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_9_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_9_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_9_grad/Identity:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_9_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_8/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_8/embedding_lookup_grad/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_8/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_7_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_7_grad/gradients/grad_ys_0_values:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_7_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_7_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_7_grad/Identity:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_7_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_6/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_6/embedding_lookup_grad/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_6/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_5_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_5_grad/gradients/grad_ys_0_values:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_5_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_5_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_5_grad/Identity:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_5_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_4/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_4/embedding_lookup_grad/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_4/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_3_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_3_grad/gradients/grad_ys_0_values:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_3_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_3_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_3_grad/Identity:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_3_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_1_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_1_grad/gradients/grad_ys_0_values:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_1_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_1_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_1_grad/Identity:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/cond_1_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradients/rgcn/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1330/1330 [==============================] - 39s 24ms/step - loss: 0.3029 - auc: 0.4407 - val_loss: 0.2424 - val_auc: 0.4739\n",
      "Epoch 2/10\n",
      "1330/1330 [==============================] - 26s 19ms/step - loss: 0.2367 - auc: 0.4440 - val_loss: 0.2226 - val_auc: 0.4284\n",
      "Epoch 3/10\n",
      "1330/1330 [==============================] - 25s 19ms/step - loss: 0.2114 - auc: 0.4481 - val_loss: 0.1943 - val_auc: 0.5242\n",
      "Epoch 4/10\n",
      "1330/1330 [==============================] - 26s 19ms/step - loss: 0.2018 - auc: 0.4456 - val_loss: 0.1960 - val_auc: 0.4971\n",
      "Epoch 5/10\n",
      "1330/1330 [==============================] - 25s 19ms/step - loss: 0.2039 - auc: 0.4754 - val_loss: 0.1958 - val_auc: 0.5631\n",
      "Epoch 6/10\n",
      "1330/1330 [==============================] - 25s 19ms/step - loss: 0.1974 - auc: 0.4905 - val_loss: 0.1920 - val_auc: 0.4824\n",
      "Epoch 7/10\n",
      "1330/1330 [==============================] - 25s 19ms/step - loss: 0.1993 - auc: 0.5350 - val_loss: 0.1897 - val_auc: 0.5663\n",
      "Epoch 8/10\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 0.2002 - auc: 0.5425 - val_loss: 0.1874 - val_auc: 0.5798\n",
      "Epoch 9/10\n",
      "1330/1330 [==============================] - 26s 19ms/step - loss: 0.1965 - auc: 0.5490 - val_loss: 0.1930 - val_auc: 0.5907\n",
      "Epoch 10/10\n",
      "1330/1330 [==============================] - 25s 19ms/step - loss: 0.1964 - auc: 0.5631 - val_loss: 0.1934 - val_auc: 0.5283\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3fcfd6670>"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "# fit in the data\n",
    "model1.fit(\n",
    "    gen_batch(\n",
    "        training_set, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=10,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNH3Kx9jAJXJ",
    "outputId": "6d94c5f0-3c4d-417a-cfa8-bd6ecc0fe3df"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_11/StatefulPartitionedCall:0', description=\"created by layer 'gnn_11'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_11/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_11'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_11/Sigmoid:0', description=\"created by layer 'dense_11'\")\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_11 (TFOpLam  ()                  0           ['input_36[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)       (None, 20)           10000       ['input_34[0][0]']               \n",
      "                                                                                                  \n",
      " input_35 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  ()                  0           ['tf.math.reduce_max_11[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_11 (GNN)                   (None, 16)           7648        ['embedding_11[0][0]',           \n",
      "                                                                  'input_35[0][0]',               \n",
      "                                                                  'input_36[0][0]',               \n",
      "                                                                  'tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_11 (TFOpL  (None, 16)          0           ['gnn_11[0][0]',                 \n",
      " ambda)                                                           'input_36[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            17          ['tf.math.segment_mean_11[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,665\n",
      "Trainable params: 17,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model1_2 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model1_2.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNbW0qLZAJmH",
    "outputId": "cd623b0c-9e07-420f-a618-e38c892a9e26"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1330/1330 [==============================] - 17s 11ms/step - loss: 0.2089 - auc: 0.5623 - val_loss: 0.1888 - val_auc: 0.6426\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1894 - auc: 0.6315 - val_loss: 0.1834 - val_auc: 0.6783\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1893 - auc: 0.6320 - val_loss: 0.1939 - val_auc: 0.6777\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 0.1872 - auc: 0.6555 - val_loss: 0.1876 - val_auc: 0.6737\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1874 - auc: 0.6488 - val_loss: 0.1802 - val_auc: 0.6994\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1873 - auc: 0.6602 - val_loss: 0.1753 - val_auc: 0.7097\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1852 - auc: 0.6741 - val_loss: 0.1853 - val_auc: 0.7080\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1840 - auc: 0.6795 - val_loss: 0.1855 - val_auc: 0.6941\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 0.1843 - auc: 0.6754 - val_loss: 0.1852 - val_auc: 0.7119\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1849 - auc: 0.6747 - val_loss: 0.1774 - val_auc: 0.7206\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1841 - auc: 0.6768 - val_loss: 0.1787 - val_auc: 0.7241\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1840 - auc: 0.6818 - val_loss: 0.1755 - val_auc: 0.7233\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1851 - auc: 0.6738 - val_loss: 0.1878 - val_auc: 0.6865\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1851 - auc: 0.6655 - val_loss: 0.1732 - val_auc: 0.7325\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1862 - auc: 0.6654 - val_loss: 0.1735 - val_auc: 0.6976\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1847 - auc: 0.6787 - val_loss: 0.1832 - val_auc: 0.7200\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1846 - auc: 0.6785 - val_loss: 0.1837 - val_auc: 0.7028\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1840 - auc: 0.6804 - val_loss: 0.1743 - val_auc: 0.6946\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1857 - auc: 0.6639 - val_loss: 0.1814 - val_auc: 0.7222\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.1869 - auc: 0.6591 - val_loss: 0.1802 - val_auc: 0.6915\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3f4c65fa0>"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "model1_2.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "# fit in the data\n",
    "model1_2.fit(\n",
    "    gen_batch(\n",
    "        training_set, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdKk0ATJU7Pp"
   },
   "source": [
    "### tiral 2  up-sample the positive class samples \n",
    "\n",
    "The prior training result reveals no significant improvement over the model. So we plan to  up-sample the positive class samples to rebalance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c83b705362e9464b85b98b46f00636c6",
      "35f4c10ff879455d8e33a64db00c3a80",
      "f42d684d747147928cfc4d0c137ffac0",
      "1b5ffdecd026473d8a1cd3990dc2d1ef",
      "ce259a9c2f1344a8859aef3425656b1f",
      "a59f66fe8c784aa9bd1508ad0502206a",
      "3ca6130cc5b441c8b022e2b4cb098713",
      "7bd8dcc715134724b5b552915a27e2bb",
      "d5ec9377509547a7ae7a0257c4dd1060",
      "bd6a878ab0d54d0a95d0a29e0788ce88",
      "3b0aedb01090432e919d1fce356c1a5c"
     ]
    },
    "id": "FtljDcwsCLP5",
    "outputId": "7082c7f6-4526-411a-873a-2ec231290e8f"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/25024 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c83b705362e9464b85b98b46f00636c6"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # import library\n",
    "\n",
    "training_set = read_sdf('/content/drive/MyDrive/a6/train.sdf') # read in traning data\n",
    "\n",
    "training_set, validation_set = train_test_split(training_set, test_size=0.15,) # split traning and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "bepF-uHjCLYu",
    "outputId": "c9beaac4-ef70-4075-e908-bd5966ac9640"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-77-7e8cd95110ed>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sns.countplot(np.array(training_set)[:,2]) # map the two classes' distribution\n",
      "/usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa3f3479f70>"
      ]
     },
     "metadata": {},
     "execution_count": 77
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(np.array(training_set)[:,2]) # map the two classes' distribution \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qHPQ5c7FUjg"
   },
   "source": [
    "The summary plot shows that the training data is significantly  imbalanced. the number of sample with 0 label is approximately 20 times that of samples with label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48PpYhHFOylR",
    "outputId": "18487667-a842-4f58-b774-eb837bd6598c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20247\n",
      "1023\n"
     ]
    }
   ],
   "source": [
    "train_neg =[i for i in training_set if i[2] ==0] # loop over to get all 0class samples from training_set\n",
    "train_pos =[i for i in training_set if i[2] ==1] # loop over to get all 1class samples from training_set\n",
    "print(len(train_neg))\n",
    "print(len(train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzUGgPaMSqhF",
    "outputId": "d334a928-5007-4aa3-fa4e-41627ea40c09"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20247\n",
      "20247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample \n",
    "train_pos = resample(train_pos, replace=True, n_samples=len(train_neg))\n",
    "print(len(train_neg))\n",
    "print(len(train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "5o2QNM7ZSXY2"
   },
   "outputs": [],
   "source": [
    "tranin_upsampling= train_neg + train_pos # concatenate the positive and negtive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "ls-PRruNWofO",
    "outputId": "2e0f2e2f-5bcd-4d3b-ea96-680808f2f805"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-81-560ec66e607d>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sns.countplot(np.array(tranin_upsampling)[:,2])  # map the two classes' distribution\n",
      "/usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40494\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa4fb214d00>"
      ]
     },
     "metadata": {},
     "execution_count": 81
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "print(len(tranin_upsampling))\n",
    "import seaborn as sns\n",
    "sns.countplot(np.array(tranin_upsampling)[:,2])  # map the two classes' distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92lDI0Wtfkvs",
    "outputId": "f9793fbd-81a3-444f-8d03-26f3d3ca3245"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_3/StatefulPartitionedCall:0', description=\"created by layer 'gnn_3'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_3/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_3'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_3/Sigmoid:0', description=\"created by layer 'dense_3'\")\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_3 (TFOpLamb  ()                  0           ['input_12[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 20)           10000       ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  ()                  0           ['tf.math.reduce_max_3[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_3 (GNN)                    (None, 16)           7648        ['embedding_3[0][0]',            \n",
      "                                                                  'input_11[0][0]',               \n",
      "                                                                  'input_12[0][0]',               \n",
      "                                                                  'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_3 (TFOpLa  (None, 16)          0           ['gnn_3[0][0]',                  \n",
      " mbda)                                                            'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            17          ['tf.math.segment_mean_3[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,665\n",
      "Trainable params: 17,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model2_1 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model2_1.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COdhGb72U9sR",
    "outputId": "93f7c628-463a-4c9e-bf81-04fb359bf2bd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_3_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_3_grad/gradients/grad_ys_0_values:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/cond_3_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_3_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_3_grad/Identity:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/cond_3_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_1_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_1_grad/gradients/grad_ys_0_values:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/cond_1_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_1_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_1_grad/Identity:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/cond_1_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/rgcn/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1330/1330 [==============================] - 17s 11ms/step - loss: 0.6615 - auc: 0.6289 - val_loss: 0.7346 - val_auc: 0.6298\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.6441 - auc: 0.6667 - val_loss: 0.5740 - val_auc: 0.6318\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.6280 - auc: 0.6908 - val_loss: 0.7013 - val_auc: 0.7004\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.6179 - auc: 0.7144 - val_loss: 0.7115 - val_auc: 0.6997\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.6126 - auc: 0.7228 - val_loss: 0.5910 - val_auc: 0.7104\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.6103 - auc: 0.7221 - val_loss: 0.6310 - val_auc: 0.7027\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 0.6013 - auc: 0.7347 - val_loss: 0.6977 - val_auc: 0.7135\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 0.6035 - auc: 0.7322 - val_loss: 0.6951 - val_auc: 0.7162\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.6049 - auc: 0.7314 - val_loss: 0.5372 - val_auc: 0.7281\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5965 - auc: 0.7414 - val_loss: 0.6413 - val_auc: 0.7361\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5977 - auc: 0.7407 - val_loss: 0.7531 - val_auc: 0.7344\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 15s 12ms/step - loss: 0.5938 - auc: 0.7428 - val_loss: 0.7205 - val_auc: 0.7234\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5949 - auc: 0.7444 - val_loss: 0.6159 - val_auc: 0.7503\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5852 - auc: 0.7541 - val_loss: 0.5802 - val_auc: 0.7431\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5873 - auc: 0.7530 - val_loss: 0.7067 - val_auc: 0.7534\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5876 - auc: 0.7543 - val_loss: 0.7676 - val_auc: 0.7450\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5803 - auc: 0.7628 - val_loss: 0.7053 - val_auc: 0.7584\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5824 - auc: 0.7601 - val_loss: 0.7259 - val_auc: 0.7479\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5740 - auc: 0.7689 - val_loss: 0.7096 - val_auc: 0.7531\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 0.5740 - auc: 0.7682 - val_loss: 0.7065 - val_auc: 0.7738\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa408922ca0>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "model2_1.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "# fit in the data\n",
    "model2_1.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9PIXYYQf4vc"
   },
   "source": [
    "### trial 3-4 Class Weighs\n",
    "\n",
    "The upsampling strategy appears to have enhanced the model's performance. The drawback of this method is that it generates a large number of repeated samples in the training data. Instead, let's use the original training data but give the minority class greater weight when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "52886d1296b9439db3cb7dee98208b94",
      "8e39890ad2d5416da18cd274c8529827",
      "170239e2175c4ec5a0146e525d413b23",
      "6806d751d95347679b5fe1813c76142d",
      "a487ab64812843bfb1519a7e44a8cc5f",
      "c524fdeb4b5a4359bd4d570f6fc77ecb",
      "a4e5ae75f9134b6a9672bccd942522a8",
      "03920e8109df487f9c770e8e76147d72",
      "57cb06d7781448aaa2235b8639d2dbdd",
      "637541ffb379411eae93c1ea0a1a8c41",
      "056e7a8e1df44ab6a4037e536be9e6c8"
     ]
    },
    "id": "0Gy0OL6qZQHB",
    "outputId": "1a0a0cd3-1347-4fa9-ce88-a24fa8b8733d"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/25024 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52886d1296b9439db3cb7dee98208b94"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # import library\n",
    "\n",
    "training_set = read_sdf('/content/drive/MyDrive/a6/train.sdf') # read in traning data\n",
    "\n",
    "training_set, validation_set = train_test_split(training_set, test_size=0.15,) # split traning and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "i6jmkHtwZZy3",
    "outputId": "862ffe6f-94dd-4346-89c3-2940f49ab121"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-34-b056b9c74403>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sns.countplot(np.array(training_set)[:,2]) # map the two classes' distribution\n",
      "/usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa4081a3070>"
      ]
     },
     "metadata": {},
     "execution_count": 34
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(np.array(training_set)[:,2]) # map the two classes' distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhWS_tf5aNqa",
    "outputId": "fb567584-532b-46a9-ce1c-152807199954"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20226\n",
      "1044\n"
     ]
    }
   ],
   "source": [
    "train_neg =[i for i in training_set if i[2] ==0] # loop over to get all the 0 class samples from the dataset\n",
    "train_pos =[i for i in training_set if i[2] ==1] # loop over to get all the 1 class samples from the dataset\n",
    "print(len(train_neg))\n",
    "print(len(train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJDfp3QkZrV5",
    "outputId": "ccf2ba85-9c1b-4a38-87d6-1f51be293eb7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_4/StatefulPartitionedCall:0', description=\"created by layer 'gnn_4'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_4/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_4'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_4/Sigmoid:0', description=\"created by layer 'dense_4'\")\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_13 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_4 (TFOpLamb  ()                  0           ['input_15[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 20)           10000       ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  ()                  0           ['tf.math.reduce_max_4[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_4 (GNN)                    (None, 16)           7648        ['embedding_4[0][0]',            \n",
      "                                                                  'input_14[0][0]',               \n",
      "                                                                  'input_15[0][0]',               \n",
      "                                                                  'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_4 (TFOpLa  (None, 16)          0           ['gnn_4[0][0]',                  \n",
      " mbda)                                                            'input_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            17          ['tf.math.segment_mean_4[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,665\n",
      "Trainable params: 17,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model3 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model3.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HInS27-2ZrZ5",
    "outputId": "2f6bc371-00a3-4be4-93dd-fdb6aefec07a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1330/1330 [==============================] - 16s 11ms/step - loss: 1.2660 - auc: 0.6459 - val_loss: 0.6171 - val_auc: 0.6381\n",
      "Epoch 2/10\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.2608 - auc: 0.6444 - val_loss: 0.7005 - val_auc: 0.6582\n",
      "Epoch 3/10\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.2717 - auc: 0.6358 - val_loss: 0.5725 - val_auc: 0.6170\n",
      "Epoch 4/10\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.2719 - auc: 0.6447 - val_loss: 0.5549 - val_auc: 0.6204\n",
      "Epoch 5/10\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.2739 - auc: 0.6402 - val_loss: 0.7609 - val_auc: 0.6211\n",
      "Epoch 6/10\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.2641 - auc: 0.6455 - val_loss: 0.6338 - val_auc: 0.6748\n",
      "Epoch 7/10\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.2627 - auc: 0.6475 - val_loss: 0.5515 - val_auc: 0.6095\n",
      "Epoch 8/10\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.2716 - auc: 0.6423 - val_loss: 0.7580 - val_auc: 0.6884\n",
      "Epoch 9/10\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.2620 - auc: 0.6474 - val_loss: 0.6861 - val_auc: 0.6605\n",
      "Epoch 10/10\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.2645 - auc: 0.6472 - val_loss: 0.5725 - val_auc: 0.6893\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3f943ec70>"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model3.fit(\n",
    "    gen_batch(\n",
    "        training_set, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=10,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "It appears that this strategy significantly improves the model's performance. Let's experiment with upsampling and class weights to see how they affect the model. "
   ],
   "metadata": {
    "id": "n3286OscPbj6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model3_2 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model3_2.summary() #printing summary of the model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_AeiQs5QC1A",
    "outputId": "d131e2b0-9774-43b8-c04d-b356fe52bb7d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_5/StatefulPartitionedCall:0', description=\"created by layer 'gnn_5'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_5/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_5'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_5/Sigmoid:0', description=\"created by layer 'dense_5'\")\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_5 (TFOpLamb  ()                  0           ['input_18[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 20)           10000       ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  ()                  0           ['tf.math.reduce_max_5[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_5 (GNN)                    (None, 16)           7648        ['embedding_5[0][0]',            \n",
      "                                                                  'input_17[0][0]',               \n",
      "                                                                  'input_18[0][0]',               \n",
      "                                                                  'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_5 (TFOpLa  (None, 16)          0           ['gnn_5[0][0]',                  \n",
      " mbda)                                                            'input_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            17          ['tf.math.segment_mean_5[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,665\n",
      "Trainable params: 17,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model3_2.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model3_2.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ") "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eM2YFwXQGFi",
    "outputId": "f85f6399-ee25-4564-9e33-67b4dab35cf0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1330/1330 [==============================] - 17s 11ms/step - loss: 2.0860 - auc: 0.6278 - val_loss: 3.3162 - val_auc: 0.6823\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.8972 - auc: 0.6845 - val_loss: 3.6011 - val_auc: 0.6884\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.8887 - auc: 0.6990 - val_loss: 3.3881 - val_auc: 0.7202\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8671 - auc: 0.7038 - val_loss: 3.2647 - val_auc: 0.7300\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.8345 - auc: 0.7297 - val_loss: 3.0172 - val_auc: 0.7725\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.8301 - auc: 0.7357 - val_loss: 2.7025 - val_auc: 0.7795\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.8093 - auc: 0.7403 - val_loss: 2.9601 - val_auc: 0.7667\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.8043 - auc: 0.7474 - val_loss: 3.0209 - val_auc: 0.7685\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.8051 - auc: 0.7447 - val_loss: 3.0098 - val_auc: 0.8006\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.7969 - auc: 0.7517 - val_loss: 2.8792 - val_auc: 0.7982\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.7924 - auc: 0.7542 - val_loss: 3.3230 - val_auc: 0.7907\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.7885 - auc: 0.7551 - val_loss: 3.0728 - val_auc: 0.7899\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 14s 11ms/step - loss: 1.7851 - auc: 0.7615 - val_loss: 3.1375 - val_auc: 0.8086\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.7872 - auc: 0.7539 - val_loss: 2.6880 - val_auc: 0.8045\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.7819 - auc: 0.7621 - val_loss: 3.0798 - val_auc: 0.7803\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.7944 - auc: 0.7585 - val_loss: 2.9759 - val_auc: 0.7891\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.7730 - auc: 0.7665 - val_loss: 3.0736 - val_auc: 0.7840\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.7672 - auc: 0.7654 - val_loss: 2.9709 - val_auc: 0.7865\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.7824 - auc: 0.7610 - val_loss: 3.0692 - val_auc: 0.8051\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 15s 11ms/step - loss: 1.7597 - auc: 0.7707 - val_loss: 3.0259 - val_auc: 0.8140\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4061c07f0>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KepkyV7ccxTw"
   },
   "source": [
    "It appears that combining upsampling and class weights has considerably enhanced the model's performance to 80% above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvvvRi0zczXR"
   },
   "source": [
    "### Trial 5-10 Different GCN aggregation mechanisms\n",
    "\n",
    "According to the prior training results, both upsampling and the class weights strategy significantly enhanced the model's performance. Let's experiment with different GCN aggregation mechanisms to observe how they affect model performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVGexQ2Ade60"
   },
   "outputs": [],
   "source": [
    "from tf2_gnn.layers.gnn import GNN, GNNInput #import the model\n",
    "from  tf2_gnn.layers.message_passing import gnn_edge_mlp,gnn_film,rgat,rgin,rgcn,ggnn # import GCN aggregation mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUlu2dHm3-ys"
   },
   "source": [
    "#### GGNN\n",
    "\n",
    "Gated Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zwaLWqXde9h",
    "outputId": "394b3d1e-0d9c-43fc-8535-0715ee454d52"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_5/StatefulPartitionedCall:0', description=\"created by layer 'gnn_5'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_5/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_5'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_5/Sigmoid:0', description=\"created by layer 'dense_5'\")\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_5 (TFOpLamb  ()                  0           ['input_18[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 20)           10000       ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  ()                  0           ['tf.math.reduce_max_5[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_5 (GNN)                    (None, 16)           14176       ['embedding_5[0][0]',            \n",
      "                                                                  'input_17[0][0]',               \n",
      "                                                                  'input_18[0][0]',               \n",
      "                                                                  'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_5 (TFOpLa  (None, 16)          0           ['gnn_5[0][0]',                  \n",
      " mbda)                                                            'input_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            17          ['tf.math.segment_mean_5[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,193\n",
      "Trainable params: 24,193\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'GGNN'\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model4_1 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model4_1.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6JFx2o5dfAI",
    "outputId": "59e3b76a-d543-4e34-dd1b-5780308c20c2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/ggnn_3/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/ggnn_3/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/ggnn_3/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/ggnn_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/ggnn_2/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/ggnn_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/ggnn_1/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/ggnn_1/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/ggnn_1/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/ggnn/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/ggnn/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/ggnn/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1330/1330 [==============================] - 23s 15ms/step - loss: 1.9973 - auc: 0.6404 - val_loss: 2.9266 - val_auc: 0.7154\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.8901 - auc: 0.6867 - val_loss: 2.3202 - val_auc: 0.7079\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.8599 - auc: 0.7024 - val_loss: 3.0713 - val_auc: 0.7413\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 19s 15ms/step - loss: 1.8518 - auc: 0.7095 - val_loss: 2.8798 - val_auc: 0.7481\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.8541 - auc: 0.7154 - val_loss: 2.9835 - val_auc: 0.7651\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.8232 - auc: 0.7334 - val_loss: 2.7253 - val_auc: 0.7816\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.8253 - auc: 0.7314 - val_loss: 2.5287 - val_auc: 0.7821\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.8043 - auc: 0.7439 - val_loss: 2.5723 - val_auc: 0.7761\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7949 - auc: 0.7501 - val_loss: 2.2600 - val_auc: 0.8017\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7863 - auc: 0.7543 - val_loss: 2.2743 - val_auc: 0.8046\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7641 - auc: 0.7651 - val_loss: 2.3386 - val_auc: 0.8177\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7466 - auc: 0.7668 - val_loss: 2.2878 - val_auc: 0.8195\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7527 - auc: 0.7731 - val_loss: 2.3226 - val_auc: 0.8098\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7497 - auc: 0.7773 - val_loss: 2.2755 - val_auc: 0.8132\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7232 - auc: 0.7803 - val_loss: 2.4290 - val_auc: 0.8280\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 19s 15ms/step - loss: 1.7291 - auc: 0.7822 - val_loss: 2.2230 - val_auc: 0.8149\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7234 - auc: 0.7806 - val_loss: 2.2129 - val_auc: 0.8301\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7161 - auc: 0.7861 - val_loss: 2.0271 - val_auc: 0.8224\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7128 - auc: 0.7883 - val_loss: 2.0804 - val_auc: 0.8312\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.7032 - auc: 0.7918 - val_loss: 2.0910 - val_auc: 0.8023\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcc27d91c0>"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "model4_1.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model4_1.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpDSm_fz34nM"
   },
   "source": [
    "#### RGCN\n",
    "\n",
    "Relational Graph Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jr-yj7HM4Fkh",
    "outputId": "c6d6a10f-4667-499c-9a73-0ce057130053"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_6/StatefulPartitionedCall:0', description=\"created by layer 'gnn_6'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_6/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_6'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_6/Sigmoid:0', description=\"created by layer 'dense_6'\")\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_6 (TFOpLamb  ()                  0           ['input_21[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 20)           10000       ['input_19[0][0]']               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  ()                  0           ['tf.math.reduce_max_6[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_6 (GNN)                    (None, 16)           7648        ['embedding_6[0][0]',            \n",
      "                                                                  'input_20[0][0]',               \n",
      "                                                                  'input_21[0][0]',               \n",
      "                                                                  'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_6 (TFOpLa  (None, 16)          0           ['gnn_6[0][0]',                  \n",
      " mbda)                                                            'input_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            17          ['tf.math.segment_mean_6[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,665\n",
      "Trainable params: 17,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'RGCN'\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model4_2 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model4_2.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R9Vk49I4FnY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aba78457-afdb-4c0f-d651-0582ec8840e5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1330/1330 [==============================] - 19s 12ms/step - loss: 2.0995 - auc: 0.6159 - val_loss: 3.0262 - val_auc: 0.6817\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.9248 - auc: 0.6613 - val_loss: 3.2475 - val_auc: 0.6975\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8939 - auc: 0.6857 - val_loss: 3.0468 - val_auc: 0.7192\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8844 - auc: 0.7025 - val_loss: 2.9321 - val_auc: 0.7541\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8440 - auc: 0.7215 - val_loss: 2.8413 - val_auc: 0.7669\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8360 - auc: 0.7311 - val_loss: 2.7113 - val_auc: 0.7737\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8261 - auc: 0.7363 - val_loss: 2.7099 - val_auc: 0.7702\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8168 - auc: 0.7455 - val_loss: 2.7039 - val_auc: 0.7876\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7969 - auc: 0.7514 - val_loss: 2.3808 - val_auc: 0.7876\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7892 - auc: 0.7515 - val_loss: 2.7125 - val_auc: 0.7841\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7888 - auc: 0.7577 - val_loss: 2.8389 - val_auc: 0.7927\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7794 - auc: 0.7608 - val_loss: 2.5726 - val_auc: 0.7831\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7815 - auc: 0.7608 - val_loss: 2.5413 - val_auc: 0.7952\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7650 - auc: 0.7680 - val_loss: 2.7513 - val_auc: 0.7972\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7752 - auc: 0.7668 - val_loss: 2.6304 - val_auc: 0.8006\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7738 - auc: 0.7663 - val_loss: 2.7923 - val_auc: 0.7584\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7557 - auc: 0.7699 - val_loss: 2.8286 - val_auc: 0.7860\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7573 - auc: 0.7704 - val_loss: 2.8383 - val_auc: 0.7927\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7536 - auc: 0.7737 - val_loss: 2.6345 - val_auc: 0.8038\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7467 - auc: 0.7745 - val_loss: 2.7348 - val_auc: 0.8096\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcae5f5850>"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "model4_2.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model4_2.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq8BUdqX4O8O"
   },
   "source": [
    "#### RGAT\n",
    "\n",
    "Relational Graph Attention Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJk8zO1c4UgM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cd2d4494-604b-4657-e55c-bfb22081aec8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_6/StatefulPartitionedCall:0', description=\"created by layer 'gnn_6'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_6/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_6'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_6/Sigmoid:0', description=\"created by layer 'dense_6'\")\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_6 (TFOpLamb  ()                  0           ['input_21[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 20)           10000       ['input_19[0][0]']               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  ()                  0           ['tf.math.reduce_max_6[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_6 (GNN)                    (None, 16)           7776        ['embedding_6[0][0]',            \n",
      "                                                                  'input_20[0][0]',               \n",
      "                                                                  'input_21[0][0]',               \n",
      "                                                                  'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_6 (TFOpLa  (None, 16)          0           ['gnn_6[0][0]',                  \n",
      " mbda)                                                            'input_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            17          ['tf.math.segment_mean_6[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,793\n",
      "Trainable params: 17,793\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'RGAT'\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"num_heads\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model4_3 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model4_3.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXnbeY8Y4UjM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f2c3bd06-ccf5-4d97-e54a-7451206afcba"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_1_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_1_grad/Reshape:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_1_grad/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_grad/Reshape:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_grad/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/concat_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/concat_2:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/rgat_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgat/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgat/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/rgat/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgat/embedding_lookup_1_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgat/embedding_lookup_1_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/rgat/embedding_lookup_1_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1330/1330 [==============================] - 32s 20ms/step - loss: 2.0698 - auc: 0.6254 - val_loss: 3.1305 - val_auc: 0.6684\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.9108 - auc: 0.6758 - val_loss: 3.0976 - val_auc: 0.7078\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.8777 - auc: 0.7018 - val_loss: 3.0542 - val_auc: 0.7317\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.8413 - auc: 0.7243 - val_loss: 2.8739 - val_auc: 0.7601\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.8178 - auc: 0.7390 - val_loss: 2.5251 - val_auc: 0.7694\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.8007 - auc: 0.7547 - val_loss: 2.8450 - val_auc: 0.7888\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.7942 - auc: 0.7563 - val_loss: 2.5547 - val_auc: 0.7928\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7836 - auc: 0.7596 - val_loss: 2.6361 - val_auc: 0.8060\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7753 - auc: 0.7639 - val_loss: 2.8447 - val_auc: 0.7961\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7665 - auc: 0.7684 - val_loss: 2.5903 - val_auc: 0.8130\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7576 - auc: 0.7696 - val_loss: 2.5656 - val_auc: 0.7912\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7716 - auc: 0.7735 - val_loss: 2.6350 - val_auc: 0.8151\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7453 - auc: 0.7768 - val_loss: 2.6055 - val_auc: 0.8164\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7563 - auc: 0.7757 - val_loss: 2.6992 - val_auc: 0.7956\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7576 - auc: 0.7755 - val_loss: 2.5025 - val_auc: 0.8054\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7666 - auc: 0.7726 - val_loss: 2.8427 - val_auc: 0.8070\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7435 - auc: 0.7792 - val_loss: 2.5638 - val_auc: 0.8101\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7457 - auc: 0.7797 - val_loss: 2.3016 - val_auc: 0.7979\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7547 - auc: 0.7738 - val_loss: 2.3899 - val_auc: 0.8140\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.7498 - auc: 0.7782 - val_loss: 2.6361 - val_auc: 0.8125\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3fa401d00>"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "model4_3.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model4_3.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYjKEpx74Ybx"
   },
   "source": [
    "#### RGIN\n",
    "\n",
    "Relational Graph Isomorphism Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owp7FUo14a7V",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "01af61af-ee50-4f1f-c2a2-d3b924f82890"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_7/StatefulPartitionedCall:0', description=\"created by layer 'gnn_7'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_7/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_7'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_7/Sigmoid:0', description=\"created by layer 'dense_7'\")\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_7 (TFOpLamb  ()                  0           ['input_24[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 20)           10000       ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  ()                  0           ['tf.math.reduce_max_7[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_7 (GNN)                    (None, 16)           12768       ['embedding_7[0][0]',            \n",
      "                                                                  'input_23[0][0]',               \n",
      "                                                                  'input_24[0][0]',               \n",
      "                                                                  'tf.__operators__.add_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_7 (TFOpLa  (None, 16)          0           ['gnn_7[0][0]',                  \n",
      " mbda)                                                            'input_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            17          ['tf.math.segment_mean_7[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,785\n",
      "Trainable params: 22,785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'RGIN'\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"num_aggr_MLP_hidden_layers\"]=4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model4_4 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model4_4.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "U1yG5vL54a-Q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "80558fda-91a1-4c59-a384-3a151ec0786f"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgin_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgin_2/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/rgin_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgin/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgin/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/rgin/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1330/1330 [==============================] - 73s 15ms/step - loss: 2.0373 - auc: 0.6347 - val_loss: 2.8772 - val_auc: 0.6766\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 18s 14ms/step - loss: 1.9115 - auc: 0.6680 - val_loss: 2.6951 - val_auc: 0.6824\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 17s 13ms/step - loss: 1.8946 - auc: 0.6839 - val_loss: 3.1600 - val_auc: 0.6901\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.9127 - auc: 0.6769 - val_loss: 3.0322 - val_auc: 0.6898\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 20s 15ms/step - loss: 1.8925 - auc: 0.6921 - val_loss: 2.5830 - val_auc: 0.7378\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.8660 - auc: 0.7120 - val_loss: 2.6553 - val_auc: 0.7389\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 17s 13ms/step - loss: 1.8890 - auc: 0.7102 - val_loss: 2.9937 - val_auc: 0.7073\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 18s 13ms/step - loss: 1.8877 - auc: 0.7065 - val_loss: 2.8543 - val_auc: 0.7363\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 18s 13ms/step - loss: 1.8823 - auc: 0.7116 - val_loss: 2.7429 - val_auc: 0.7319\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 18s 13ms/step - loss: 1.8777 - auc: 0.7167 - val_loss: 2.5034 - val_auc: 0.7435\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 20s 15ms/step - loss: 1.8845 - auc: 0.7121 - val_loss: 2.7516 - val_auc: 0.7612\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.8538 - auc: 0.7243 - val_loss: 2.9393 - val_auc: 0.7583\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 19s 14ms/step - loss: 1.8810 - auc: 0.7089 - val_loss: 3.0876 - val_auc: 0.7154\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 17s 13ms/step - loss: 1.8797 - auc: 0.7072 - val_loss: 2.9886 - val_auc: 0.7387\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 18s 14ms/step - loss: 1.8555 - auc: 0.7244 - val_loss: 2.8811 - val_auc: 0.7320\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 18s 14ms/step - loss: 1.8504 - auc: 0.7250 - val_loss: 2.8900 - val_auc: 0.7537\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 18s 13ms/step - loss: 1.8378 - auc: 0.7326 - val_loss: 3.1130 - val_auc: 0.6792\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 17s 13ms/step - loss: 1.8456 - auc: 0.7310 - val_loss: 3.3813 - val_auc: 0.7293\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 18s 14ms/step - loss: 1.8599 - auc: 0.7182 - val_loss: 2.7785 - val_auc: 0.7503\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 18s 13ms/step - loss: 1.8338 - auc: 0.7268 - val_loss: 2.8486 - val_auc: 0.7179\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3f9351f10>"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "model4_4.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model4_4.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYevRIIc4fIb"
   },
   "source": [
    "#### GNN-Edge-MLP\n",
    "\n",
    "Graph Neural Network with Edge MLPs - a variant of RGCN in which messages on edges are computed using full MLPs, not just a single layer applied to the source state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-FWygij4hgy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "71825376-b470-476f-ee58-1ade0ed4ff48"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_9/StatefulPartitionedCall:0', description=\"created by layer 'gnn_9'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_9/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_9'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_9/Sigmoid:0', description=\"created by layer 'dense_9'\")\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_9 (TFOpLamb  ()                  0           ['input_30[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 20)           10000       ['input_28[0][0]']               \n",
      "                                                                                                  \n",
      " input_29 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  ()                  0           ['tf.math.reduce_max_9[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_9 (GNN)                    (None, 16)           7648        ['embedding_9[0][0]',            \n",
      "                                                                  'input_29[0][0]',               \n",
      "                                                                  'input_30[0][0]',               \n",
      "                                                                  'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_9 (TFOpLa  (None, 16)          0           ['gnn_9[0][0]',                  \n",
      " mbda)                                                            'input_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            17          ['tf.math.segment_mean_9[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,665\n",
      "Trainable params: 17,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'gnn_edge_mlp'\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"num_aggr_MLP_hidden_layers\"]=4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model4_4 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model4_4.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paKRzyY54hj6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4d8d3856-81ac-4032-ddec-4f7897fa6852"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/gnn__edge_mlp_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/gnn__edge_mlp_2/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/gnn__edge_mlp_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/gnn__edge_mlp/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/gnn__edge_mlp/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/gnn__edge_mlp/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1330/1330 [==============================] - 19s 13ms/step - loss: 2.0718 - auc: 0.6210 - val_loss: 3.0142 - val_auc: 0.7135\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.9120 - auc: 0.6688 - val_loss: 3.1646 - val_auc: 0.7015\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8896 - auc: 0.6916 - val_loss: 2.8806 - val_auc: 0.7506\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8603 - auc: 0.7094 - val_loss: 2.7856 - val_auc: 0.7541\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8462 - auc: 0.7227 - val_loss: 2.8868 - val_auc: 0.7506\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8545 - auc: 0.7230 - val_loss: 2.9851 - val_auc: 0.7317\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8309 - auc: 0.7283 - val_loss: 2.8014 - val_auc: 0.7499\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8434 - auc: 0.7315 - val_loss: 2.7776 - val_auc: 0.7639\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8273 - auc: 0.7299 - val_loss: 2.9488 - val_auc: 0.7445\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8186 - auc: 0.7384 - val_loss: 2.9690 - val_auc: 0.7668\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8243 - auc: 0.7398 - val_loss: 2.9389 - val_auc: 0.7663\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8369 - auc: 0.7335 - val_loss: 2.8681 - val_auc: 0.7583\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8236 - auc: 0.7398 - val_loss: 2.6548 - val_auc: 0.7867\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8112 - auc: 0.7481 - val_loss: 2.8207 - val_auc: 0.7539\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8161 - auc: 0.7450 - val_loss: 2.7109 - val_auc: 0.7855\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8019 - auc: 0.7513 - val_loss: 2.5649 - val_auc: 0.7660\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8032 - auc: 0.7484 - val_loss: 2.8745 - val_auc: 0.7876\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7967 - auc: 0.7536 - val_loss: 2.4916 - val_auc: 0.7690\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.8007 - auc: 0.7522 - val_loss: 2.4450 - val_auc: 0.7959\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 16s 12ms/step - loss: 1.7844 - auc: 0.7564 - val_loss: 2.5959 - val_auc: 0.7803\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efca39b0ee0>"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "model4_4.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model4_4.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guFVqcru4lW0"
   },
   "source": [
    "#### GNN-FiLM\n",
    "\n",
    "Graph Neural Networks with Feature-wise Linear Modulation (Brockschmidt, 2019) - a new extension of RGCN with FiLM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeB1_IDu4ozd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "42505954-71da-4ead-d40f-c0ea69c86323"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_10/StatefulPartitionedCall:0', description=\"created by layer 'gnn_10'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_10/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_10'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_10/Sigmoid:0', description=\"created by layer 'dense_10'\")\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_31 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_10 (TFOpLam  ()                  0           ['input_33[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_10 (Embedding)       (None, 20)           10000       ['input_31[0][0]']               \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  ()                  0           ['tf.math.reduce_max_10[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_10 (GNN)                   (None, 16)           58848       ['embedding_10[0][0]',           \n",
      "                                                                  'input_32[0][0]',               \n",
      "                                                                  'input_33[0][0]',               \n",
      "                                                                  'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_10 (TFOpL  (None, 16)          0           ['gnn_10[0][0]',                 \n",
      " ambda)                                                           'input_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1)            17          ['tf.math.segment_mean_10[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 68,865\n",
      "Trainable params: 68,865\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'gnn_film'\n",
    "params[\"film_parameter_MLP_hidden_layers\"]=12\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"num_aggr_MLP_hidden_layers\"]=4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model4_5 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model4_5.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7XpxLZR4o2Q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0ca0457f-7841-4318-fd1e-459d1215e7f0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/concat_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/concat_2:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/gnn__fi_lm_2/embedding_lookup_1_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/gnn__fi_lm/embedding_lookup_1_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/gnn__fi_lm/embedding_lookup_1_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/gnn__fi_lm/embedding_lookup_1_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/gnn__fi_lm/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/gnn__fi_lm/embedding_lookup_grad/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradients/gnn__fi_lm/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1330/1330 [==============================] - 33s 20ms/step - loss: 2.0125 - auc: 0.6200 - val_loss: 3.0113 - val_auc: 0.7648\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8923 - auc: 0.6900 - val_loss: 3.1562 - val_auc: 0.7464\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8687 - auc: 0.7149 - val_loss: 2.4193 - val_auc: 0.7150\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8752 - auc: 0.7096 - val_loss: 2.7184 - val_auc: 0.7526\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8572 - auc: 0.7181 - val_loss: 2.7255 - val_auc: 0.7639\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.8555 - auc: 0.7153 - val_loss: 3.0711 - val_auc: 0.7787\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8576 - auc: 0.7258 - val_loss: 3.1529 - val_auc: 0.7918\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.8442 - auc: 0.7266 - val_loss: 2.8334 - val_auc: 0.7879\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8571 - auc: 0.7211 - val_loss: 2.8656 - val_auc: 0.7623\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8483 - auc: 0.7307 - val_loss: 3.2010 - val_auc: 0.7554\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8272 - auc: 0.7344 - val_loss: 3.0539 - val_auc: 0.7738\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.9755 - auc: 0.6543 - val_loss: 2.7895 - val_auc: 0.7053\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.9356 - auc: 0.6647 - val_loss: 2.5828 - val_auc: 0.7419\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8844 - auc: 0.6906 - val_loss: 2.4887 - val_auc: 0.7266\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.8904 - auc: 0.6977 - val_loss: 2.5908 - val_auc: 0.7439\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8671 - auc: 0.7149 - val_loss: 2.6247 - val_auc: 0.7899\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.9319 - auc: 0.7082 - val_loss: 2.8447 - val_auc: 0.7490\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8632 - auc: 0.7098 - val_loss: 2.8749 - val_auc: 0.7550\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 1.8301 - auc: 0.7348 - val_loss: 3.0244 - val_auc: 0.7574\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 26s 20ms/step - loss: 1.8183 - auc: 0.7362 - val_loss: 2.8726 - val_auc: 0.7616\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efca240dfa0>"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "model4_5.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model4_5.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dm3_b7lK5GXv"
   },
   "source": [
    "### Trial 11-13  Different global_exchange_mode\n",
    "\n",
    "Based on the prior training results, we can conclude that RGAT is the optimal aggregation mechanism and that it improves the model's performance. Let's try RGAT with different global exchange modes. See how it affects the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S2LXcIW6FOC"
   },
   "source": [
    "#### mean\n",
    "\n",
    "mean just computes the arithmetic mean of the node and graph-level representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syVYYTqZ5M1b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "47dc34c3-3aa3-4e4c-d660-e3896850c314"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_11/StatefulPartitionedCall:0', description=\"created by layer 'gnn_11'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_11/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_11'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_11/Sigmoid:0', description=\"created by layer 'dense_11'\")\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_11 (TFOpLam  ()                  0           ['input_36[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)       (None, 20)           10000       ['input_34[0][0]']               \n",
      "                                                                                                  \n",
      " input_35 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  ()                  0           ['tf.math.reduce_max_11[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_11 (GNN)                   (None, 16)           6144        ['embedding_11[0][0]',           \n",
      "                                                                  'input_35[0][0]',               \n",
      "                                                                  'input_36[0][0]',               \n",
      "                                                                  'tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_11 (TFOpL  (None, 16)          0           ['gnn_11[0][0]',                 \n",
      " ambda)                                                           'input_36[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            17          ['tf.math.segment_mean_11[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,161\n",
      "Trainable params: 16,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'RGAT'\n",
    "params[\"global_exchange_mode\"] = 'mean'\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_heads\"] = 4\n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"num_aggr_MLP_hidden_layers\"]=4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model5_1 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model5_1.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FTeWEqb5M4N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f7f58a13-0cd9-4ff4-9393-bde13d87043f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1330/1330 [==============================] - 33s 22ms/step - loss: 2.3015 - auc: 0.5992 - val_loss: 2.5761 - val_auc: 0.6649\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.9525 - auc: 0.6492 - val_loss: 2.3131 - val_auc: 0.7107\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.9173 - auc: 0.6655 - val_loss: 2.6079 - val_auc: 0.7266\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 29s 21ms/step - loss: 1.9034 - auc: 0.6877 - val_loss: 2.7362 - val_auc: 0.7372\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8699 - auc: 0.7111 - val_loss: 2.2308 - val_auc: 0.7662\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 29s 21ms/step - loss: 1.8735 - auc: 0.7135 - val_loss: 2.3407 - val_auc: 0.7531\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8639 - auc: 0.7203 - val_loss: 2.3209 - val_auc: 0.7635\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 29s 21ms/step - loss: 1.8544 - auc: 0.7269 - val_loss: 2.4422 - val_auc: 0.7506\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 29s 21ms/step - loss: 1.8614 - auc: 0.7239 - val_loss: 2.5757 - val_auc: 0.7727\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8560 - auc: 0.7308 - val_loss: 2.4433 - val_auc: 0.7565\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.8423 - auc: 0.7367 - val_loss: 2.5373 - val_auc: 0.7604\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8704 - auc: 0.7325 - val_loss: 2.5973 - val_auc: 0.7704\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.8662 - auc: 0.7320 - val_loss: 2.4805 - val_auc: 0.7801\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.8391 - auc: 0.7400 - val_loss: 2.5581 - val_auc: 0.7800\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.8671 - auc: 0.7368 - val_loss: 2.5458 - val_auc: 0.7659\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.8582 - auc: 0.7333 - val_loss: 2.6102 - val_auc: 0.7842\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.8353 - auc: 0.7469 - val_loss: 2.7044 - val_auc: 0.7764\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.8396 - auc: 0.7463 - val_loss: 2.4556 - val_auc: 0.7735\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 1.8484 - auc: 0.7408 - val_loss: 2.5714 - val_auc: 0.7731\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8469 - auc: 0.7483 - val_loss: 2.4089 - val_auc: 0.7862\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efca2017df0>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "model5_1.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    " \n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model5_1.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfaKZFLw6I81"
   },
   "source": [
    "#### mlp\n",
    "\n",
    "mlp computes a new representation using an MLP that gets the concatenation of node and graph level representations as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxQenZJl6MCg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bf6d678c-afa7-4365-b20a-70d43882f659"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_12/StatefulPartitionedCall:0', description=\"created by layer 'gnn_12'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_12/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_12'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_12/Sigmoid:0', description=\"created by layer 'dense_12'\")\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_37 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_12 (TFOpLam  ()                  0           ['input_39[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 20)           10000       ['input_37[0][0]']               \n",
      "                                                                                                  \n",
      " input_38 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  ()                  0           ['tf.math.reduce_max_12[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_12 (GNN)                   (None, 16)           6912        ['embedding_12[0][0]',           \n",
      "                                                                  'input_38[0][0]',               \n",
      "                                                                  'input_39[0][0]',               \n",
      "                                                                  'tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_12 (TFOpL  (None, 16)          0           ['gnn_12[0][0]',                 \n",
      " ambda)                                                           'input_39[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            17          ['tf.math.segment_mean_12[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,929\n",
      "Trainable params: 16,929\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'RGAT'\n",
    "params[\"global_exchange_mode\"] = 'mlp'\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_heads\"] = 4\n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"num_aggr_MLP_hidden_layers\"]=4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model5_2 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model5_2.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPoCAUbb6MFR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "65876940-bd99-49da-968b-ca34e3707857"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1330/1330 [==============================] - 34s 22ms/step - loss: 2.2930 - auc: 0.6101 - val_loss: 2.5171 - val_auc: 0.6827\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.9163 - auc: 0.6675 - val_loss: 2.7077 - val_auc: 0.7184\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8784 - auc: 0.7036 - val_loss: 2.4784 - val_auc: 0.7317\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8702 - auc: 0.7106 - val_loss: 2.5628 - val_auc: 0.7577\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8750 - auc: 0.7137 - val_loss: 2.7002 - val_auc: 0.7558\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8588 - auc: 0.7270 - val_loss: 2.7176 - val_auc: 0.7560\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8590 - auc: 0.7339 - val_loss: 2.3901 - val_auc: 0.7604\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8860 - auc: 0.7329 - val_loss: 2.6123 - val_auc: 0.7667\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8494 - auc: 0.7341 - val_loss: 2.3403 - val_auc: 0.7581\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.9250 - auc: 0.7356 - val_loss: 2.6964 - val_auc: 0.7726\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8623 - auc: 0.7430 - val_loss: 2.6645 - val_auc: 0.7764\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8464 - auc: 0.7407 - val_loss: 2.5617 - val_auc: 0.7746\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8205 - auc: 0.7477 - val_loss: 2.7001 - val_auc: 0.7852\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8427 - auc: 0.7459 - val_loss: 2.6136 - val_auc: 0.7872\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8265 - auc: 0.7437 - val_loss: 3.0626 - val_auc: 0.7718\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8280 - auc: 0.7500 - val_loss: 2.3178 - val_auc: 0.7769\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8208 - auc: 0.7483 - val_loss: 2.5606 - val_auc: 0.7807\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8138 - auc: 0.7539 - val_loss: 2.3841 - val_auc: 0.7824\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8161 - auc: 0.7515 - val_loss: 2.5939 - val_auc: 0.7841\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8356 - auc: 0.7536 - val_loss: 2.5702 - val_auc: 0.7786\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc9fe369a0>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "model5_2.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    " \n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model5_2.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poBY0N6p6VPh"
   },
   "source": [
    "#### gru\n",
    "\n",
    "gru uses a GRU cell that gets the old node representation as state and the graph representation as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRRMrdOa6YbN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0aff7ee3-9518-4d18-d65b-05f63b5cc28b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='gnn_13/StatefulPartitionedCall:0', description=\"created by layer 'gnn_13'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.math.segment_mean_13/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_13'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_13/Sigmoid:0', description=\"created by layer 'dense_13'\")\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_42 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_40 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_13 (TFOpLam  ()                  0           ['input_42[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)       (None, 20)           10000       ['input_40[0][0]']               \n",
      "                                                                                                  \n",
      " input_41 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  ()                  0           ['tf.math.reduce_max_13[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_13 (GNN)                   (None, 16)           7776        ['embedding_13[0][0]',           \n",
      "                                                                  'input_41[0][0]',               \n",
      "                                                                  'input_42[0][0]',               \n",
      "                                                                  'tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_13 (TFOpL  (None, 16)          0           ['gnn_13[0][0]',                 \n",
      " ambda)                                                           'input_42[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1)            17          ['tf.math.segment_mean_13[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,793\n",
      "Trainable params: 17,793\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'RGAT'\n",
    "params[\"global_exchange_mode\"] = 'gru'\n",
    "params[\"hidden_dim\"] = 16 \n",
    "params[\"num_heads\"] = 4\n",
    "params[\"num_layers\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "params[\"global_exchange_every_num_layers\"] = 3\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model5_3 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model5_3.summary() #printing summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOKJtMtQ6Ydv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d385c2b4-6548-4e7f-c073-542b576472d7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1330/1330 [==============================] - 34s 22ms/step - loss: 2.0954 - auc: 0.6341 - val_loss: 3.1631 - val_auc: 0.6955\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 30s 22ms/step - loss: 1.9119 - auc: 0.6692 - val_loss: 3.1457 - val_auc: 0.7065\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.8725 - auc: 0.7021 - val_loss: 2.7631 - val_auc: 0.7443\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 30s 22ms/step - loss: 1.8470 - auc: 0.7279 - val_loss: 2.4879 - val_auc: 0.7831\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.7974 - auc: 0.7511 - val_loss: 2.3634 - val_auc: 0.7852\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.7800 - auc: 0.7568 - val_loss: 2.7794 - val_auc: 0.7949\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.7417 - auc: 0.7767 - val_loss: 2.8286 - val_auc: 0.7976\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.7483 - auc: 0.7790 - val_loss: 2.0558 - val_auc: 0.8166\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.7329 - auc: 0.7829 - val_loss: 2.6337 - val_auc: 0.8189\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.7138 - auc: 0.7861 - val_loss: 2.4566 - val_auc: 0.8138\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.6990 - auc: 0.7960 - val_loss: 2.5208 - val_auc: 0.8124\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 30s 22ms/step - loss: 1.7140 - auc: 0.7889 - val_loss: 2.6493 - val_auc: 0.8237\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 30s 22ms/step - loss: 1.6992 - auc: 0.7934 - val_loss: 2.3738 - val_auc: 0.8254\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.6923 - auc: 0.7948 - val_loss: 2.3050 - val_auc: 0.8322\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.7026 - auc: 0.7946 - val_loss: 2.3915 - val_auc: 0.8242\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.6931 - auc: 0.7979 - val_loss: 2.1580 - val_auc: 0.8268\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.6849 - auc: 0.8017 - val_loss: 2.4607 - val_auc: 0.8158\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.6831 - auc: 0.7997 - val_loss: 2.4816 - val_auc: 0.8339\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 29s 22ms/step - loss: 1.6901 - auc: 0.8007 - val_loss: 2.4967 - val_auc: 0.8362\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 30s 22ms/step - loss: 1.6725 - auc: 0.8070 - val_loss: 2.3287 - val_auc: 0.8230\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc9b170fd0>"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "model5_3.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    " \n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model5_3.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trial 14-15 different activation function and hyperparameters\n",
    "\n",
    "Let's experiment with different activation functions, various  layer and node counts to determine how they impact the model's performance. "
   ],
   "metadata": {
    "id": "VjKVbzFWYAAg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'RGAT'\n",
    "params[\"global_exchange_mode\"] = 'gru'\n",
    "params['global_exchange_every_num_layers'] = 64\n",
    "params['global_exchange_weighting_fun'] = 'softmax'\n",
    "params[\"hidden_dim\"] = 32 \n",
    "params[\"num_layers\"] = 4\n",
    "params[\"num_heads\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model5_1 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model5_1.summary() #printing summary of the model"
   ],
   "metadata": {
    "id": "hk7jqxlvZKc3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bc413793-6aee-4cc6-cc54-74770f64de3b"
   },
   "execution_count": 82,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), name='gnn_12/StatefulPartitionedCall:0', description=\"created by layer 'gnn_12'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), name='tf.math.segment_mean_12/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_12'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_12/Sigmoid:0', description=\"created by layer 'dense_12'\")\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_37 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_12 (TFOpLam  ()                  0           ['input_39[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 20)           10000       ['input_37[0][0]']               \n",
      "                                                                                                  \n",
      " input_38 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  ()                  0           ['tf.math.reduce_max_12[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_12 (GNN)                   (None, 32)           21696       ['embedding_12[0][0]',           \n",
      "                                                                  'input_38[0][0]',               \n",
      "                                                                  'input_39[0][0]',               \n",
      "                                                                  'tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_12 (TFOpL  (None, 32)          0           ['gnn_12[0][0]',                 \n",
      " ambda)                                                           'input_39[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            33          ['tf.math.segment_mean_12[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,729\n",
      "Trainable params: 31,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "model5_1.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model5_1.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")"
   ],
   "metadata": {
    "id": "IcXQcQAMZKfx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "06a877f3-ea36-49c1-8757-1af2c62fb8c3"
   },
   "execution_count": 83,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_3_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_3_grad/gradients/grad_ys_0_values:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/cond_3_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_3_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_3_grad/Identity:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/cond_3_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/concat_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/concat_2:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgat_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_1_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_1_grad/gradients/grad_ys_0_values:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/cond_1_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_1_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_1_grad/Identity:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/cond_1_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgat/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgat/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgat/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgat/embedding_lookup_1_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgat/embedding_lookup_1_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgat/embedding_lookup_1_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1330/1330 [==============================] - 31s 21ms/step - loss: 0.6481 - auc: 0.6641 - val_loss: 0.7313 - val_auc: 0.7190\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.6132 - auc: 0.7225 - val_loss: 0.6464 - val_auc: 0.7196\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 0.6034 - auc: 0.7328 - val_loss: 0.5507 - val_auc: 0.7363\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5960 - auc: 0.7425 - val_loss: 0.6150 - val_auc: 0.7422\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5893 - auc: 0.7480 - val_loss: 0.6367 - val_auc: 0.7485\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 0.5814 - auc: 0.7587 - val_loss: 0.7429 - val_auc: 0.7569\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5781 - auc: 0.7617 - val_loss: 0.5311 - val_auc: 0.7691\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5677 - auc: 0.7739 - val_loss: 0.5196 - val_auc: 0.7778\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5596 - auc: 0.7839 - val_loss: 0.5254 - val_auc: 0.7930\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5537 - auc: 0.7891 - val_loss: 0.5102 - val_auc: 0.7937\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5455 - auc: 0.7981 - val_loss: 0.4766 - val_auc: 0.8064\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5353 - auc: 0.8069 - val_loss: 0.5151 - val_auc: 0.8058\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 0.5313 - auc: 0.8113 - val_loss: 0.5290 - val_auc: 0.8032\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 27s 21ms/step - loss: 0.5301 - auc: 0.8127 - val_loss: 0.4557 - val_auc: 0.8018\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5246 - auc: 0.8173 - val_loss: 0.4371 - val_auc: 0.8066\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5279 - auc: 0.8145 - val_loss: 0.5760 - val_auc: 0.8108\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.5168 - auc: 0.8238 - val_loss: 0.4695 - val_auc: 0.8133\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 31s 23ms/step - loss: 0.5181 - auc: 0.8228 - val_loss: 0.4330 - val_auc: 0.8116\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 27s 21ms/step - loss: 0.5152 - auc: 0.8250 - val_loss: 0.5389 - val_auc: 0.8068\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 30s 23ms/step - loss: 0.5106 - auc: 0.8286 - val_loss: 0.4160 - val_auc: 0.8129\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3eee48cd0>"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "data = keras.Input(batch_shape=(None,))  #Input nodes \n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32) #Input edges\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32) #Input node2graph\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data) #embedding nodes \n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "#define GNN inputs\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters() #get default hyperparameters\n",
    "#change default hyperparameters \n",
    "params[\"message_calculation_class\"] = 'RGAT'\n",
    "params[\"global_exchange_mode\"] = 'gru'\n",
    "params['global_exchange_every_num_layers'] = 500\n",
    "params['global_exchange_weighting_fun'] = 'softmax'\n",
    "params[\"hidden_dim\"] = 128 \n",
    "params[\"num_layers\"] = 5\n",
    "params[\"num_heads\"] = 4\n",
    "params[\"dense_every_num_layers\"] = 4\n",
    "params[\"layer_input_dropout_rate\"] = 0.2\n",
    "gnn_layer = GNN(params) #gnn layer with defined hyperparameters\n",
    "gnn_out = gnn_layer(gnn_input) #gnn output layer \n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(   #calculating segmented mean \n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg) # define the dense layer with sigmoid activation\n",
    "print('pred:', pred)\n",
    "\n",
    "#build model with input of data,edges node2graph and output of prediction\n",
    "model5_3 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model5_3.summary() #printing summary of the model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7WhU3HbhFb8",
    "outputId": "09065d2d-deab-4480-e31f-06faad76a73b"
   },
   "execution_count": 94,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 128), dtype=tf.float32, name=None), name='gnn_18/StatefulPartitionedCall:0', description=\"created by layer 'gnn_18'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 128), dtype=tf.float32, name=None), name='tf.math.segment_mean_18/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_18'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_18/Sigmoid:0', description=\"created by layer 'dense_18'\")\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_57 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_55 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_18 (TFOpLam  ()                  0           ['input_57[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_18 (Embedding)       (None, 20)           10000       ['input_55[0][0]']               \n",
      "                                                                                                  \n",
      " input_56 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  ()                  0           ['tf.math.reduce_max_18[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_18 (GNN)                   (None, 128)          118528      ['embedding_18[0][0]',           \n",
      "                                                                  'input_56[0][0]',               \n",
      "                                                                  'input_57[0][0]',               \n",
      "                                                                  'tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_18 (TFOpL  (None, 128)         0           ['gnn_18[0][0]',                 \n",
      " ambda)                                                           'input_57[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 1)            129         ['tf.math.segment_mean_18[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 128,657\n",
      "Trainable params: 128,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "model5_2.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ") # compile model with BinaryCrossentropy loss and AUC metrics\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size) # the number of batches for training data\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size) # the number of batches for validation data\n",
    "\n",
    "cw= {0:1,1:len(train_neg)/len(train_pos)}\n",
    "# fit in the data\n",
    "model5_2.fit(\n",
    "    gen_batch(\n",
    "        tranin_upsampling, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    class_weight=cw\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suj1Oo25hYnu",
    "outputId": "fd9349fb-0214-4e51-e5a6-1618c4d2448c"
   },
   "execution_count": 95,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1330/1330 [==============================] - 31s 22ms/step - loss: 0.4443 - auc: 0.8749 - val_loss: 0.5287 - val_auc: 0.8245\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4485 - auc: 0.8724 - val_loss: 0.4919 - val_auc: 0.8212\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 0.4489 - auc: 0.8724 - val_loss: 0.5758 - val_auc: 0.8235\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 27s 21ms/step - loss: 0.4451 - auc: 0.8748 - val_loss: 0.4301 - val_auc: 0.8276\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4468 - auc: 0.8736 - val_loss: 0.4460 - val_auc: 0.8352\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4415 - auc: 0.8766 - val_loss: 0.5402 - val_auc: 0.8320\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 0.4469 - auc: 0.8734 - val_loss: 0.4707 - val_auc: 0.8242\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4378 - auc: 0.8792 - val_loss: 0.5122 - val_auc: 0.8183\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4409 - auc: 0.8771 - val_loss: 0.4251 - val_auc: 0.8389\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 0.4392 - auc: 0.8785 - val_loss: 0.4721 - val_auc: 0.8262\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 0.4408 - auc: 0.8771 - val_loss: 0.4518 - val_auc: 0.8194\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4440 - auc: 0.8753 - val_loss: 0.4514 - val_auc: 0.8360\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 27s 21ms/step - loss: 0.4350 - auc: 0.8801 - val_loss: 0.4991 - val_auc: 0.8243\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 27s 21ms/step - loss: 0.4361 - auc: 0.8801 - val_loss: 0.4791 - val_auc: 0.8108\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4371 - auc: 0.8794 - val_loss: 0.4470 - val_auc: 0.8216\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4457 - auc: 0.8746 - val_loss: 0.4490 - val_auc: 0.8249\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4304 - auc: 0.8838 - val_loss: 0.5004 - val_auc: 0.8154\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 28s 21ms/step - loss: 0.4332 - auc: 0.8819 - val_loss: 0.4971 - val_auc: 0.8274\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.4362 - auc: 0.8801 - val_loss: 0.4304 - val_auc: 0.8216\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 27s 21ms/step - loss: 0.4352 - auc: 0.8808 - val_loss: 0.4404 - val_auc: 0.8324\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3e2f43550>"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4kXTegd2Ydw"
   },
   "source": [
    "## Task 5 submision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "1EKMMJXy2dMA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "88571b59fd40473480fe106eba5892ba",
      "0dcf88605b38411184ad38ec916b5937",
      "0532fd5b9af24364806077e8558dec4d",
      "4136931f09da413aa24bad84459b390d",
      "8e19ddc5e25640fa95a9e975b003bbca",
      "a2f77f2a014c47898673126fb775fecb",
      "4b308c3c9d3b44b6a284fe5f67c930f3",
      "be3ecf4e7e6643d193a96ecce0cca31c",
      "e92e98d6aa524d15896e4a2338c57687",
      "d616caad734340bdb2a9722e59e593bd",
      "68e59d5a9dad438fb101a05d4f2e66fd"
     ]
    },
    "outputId": "44aaef69-a731-43dd-c341-19bfad58c675"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/12326 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88571b59fd40473480fe106eba5892ba"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "testing_set  = read_sdf('/content/drive/MyDrive/a6/test_x.sdf')# read in testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "Qe9t_9vz2dO6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a7f2ac60-f015-4810-a73a-2ea4026bb312"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "771/771 [==============================] - 4s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model5_2.predict( # generate prediction using trained model\n",
    "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
    ") \n",
    "y_pred = np.reshape(y_pred, -1) #reshap the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "_NZI25Km2dSH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # prepare csv file for submision\n",
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "REF9X-_o2oc9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "outputId": "7d455d5e-aceb-4c03-9e85-290f6b472ab2"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "download(\"download_e5ca9f0f-58ea-4ad3-a70a-285508086c17\", \"sample_submission.csv\", 197109)"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "# from google.colab import files\n",
    "# files.download(\"sample_submission.csv\") #download the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![WeChat Screenshot_20221205191409.png](data:image/png;base64,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)"
   ],
   "metadata": {
    "id": "10e_rLlKypZl"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBpEbp7-BcsZ"
   },
   "source": [
    "## Task 6 Answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBqMu-1CBeWf"
   },
   "source": [
    "🌈Based on the provided template, describe the format of the input file (sdf file).\n",
    "\n",
    "The input file is a data structure file (SDF). It includes the compound's name, Atom count, version number, connections, and label are also shown. The atom block describes the constituents of the chemical. The edge block describes the compound's bonding structure. Both of these blocks are used to obtain information about the compound and save it in the form of edges and nodes. Each node represents an atom from the chemical molecule. Different compounds are distinguished by doller signs\n",
    "\n",
    "\n",
    "🌈What are the input tensors to the neural network model (their meaning, not just symbol)? What is each of their dims and their meaning (e.g. batch_size)?\n",
    "\n",
    "nodes: Since the nodes' data are preprocessed before being sent into the network, the chemical compound information for each node is in tokenized form. Each batch has the form [batch size*max len nodes], where batch size is the number of samples and max len nodes is the length of tokenized nodes after padding.\n",
    "\n",
    "edge: edge is the input tensor containing information about the interatomic bonding structure. The edge has the form [sum of all edges,2]. The sum of all edges is determined by summing the total number of edges for each sample in a batch.\n",
    "\n",
    "node2graph:  It is the input tensor that stores information about segmented ids. Each batch's shape is [batch size*max len nodes], where batch size is the number of samples in the batch and max len nodes is the length of tokenized nodes after padding.\n",
    "\n",
    "\n",
    "\n",
    "🌈For each dim of gnn_out, what does it symbolize? For each dim of avg, what does it symbolize?\n",
    "\n",
    "**gnn_out:** (batch size node dimension,hidden layers) is the shape of gnn out, where batch size node dimension is the tokenized vector dimension of the whole batch.\n",
    "\n",
    "\n",
    "\n",
    "**avg:** Based on the segmented ids, verage computes the segmented mean of the gnn out. The segment mean takes the mean of all the gnn out output data and represents one sample with one integer for each hidden layer. The average tensor's final output has the form [batch size, hidden layer]. It is a method of gathering information for each sample and interpreting it as mean data.\n",
    "\n",
    "\n",
    "🌈What is the difference between segment_mean and tf.reduce_mean? For each dim of pred, what does it symbolize?\n",
    "\n",
    "segment_mean takes the mean of the data which have same segmented ids.\n",
    "\n",
    "reduce_mean  computes the mean of elements across dimensions of a tensor.\n",
    "\n",
    "🌈What is the motivation/theory/idea to use multiple gcn layers comparing to just one? How many layers were used in the template?\n",
    "\n",
    "\n",
    " the GNN model from the template is implemented with  the default setting of 4  layers. The default message passing method is rgcn (Graph convolution layers). Using multiple gcn facilitates the incorporation of all graph complexity.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "toc_visible": true,
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "52886d1296b9439db3cb7dee98208b94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e39890ad2d5416da18cd274c8529827",
       "IPY_MODEL_170239e2175c4ec5a0146e525d413b23",
       "IPY_MODEL_6806d751d95347679b5fe1813c76142d"
      ],
      "layout": "IPY_MODEL_a487ab64812843bfb1519a7e44a8cc5f"
     }
    },
    "8e39890ad2d5416da18cd274c8529827": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c524fdeb4b5a4359bd4d570f6fc77ecb",
      "placeholder": "​",
      "style": "IPY_MODEL_a4e5ae75f9134b6a9672bccd942522a8",
      "value": "100%"
     }
    },
    "170239e2175c4ec5a0146e525d413b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03920e8109df487f9c770e8e76147d72",
      "max": 25024,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57cb06d7781448aaa2235b8639d2dbdd",
      "value": 25024
     }
    },
    "6806d751d95347679b5fe1813c76142d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_637541ffb379411eae93c1ea0a1a8c41",
      "placeholder": "​",
      "style": "IPY_MODEL_056e7a8e1df44ab6a4037e536be9e6c8",
      "value": " 25024/25024 [00:01&lt;00:00, 13653.34it/s]"
     }
    },
    "a487ab64812843bfb1519a7e44a8cc5f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c524fdeb4b5a4359bd4d570f6fc77ecb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4e5ae75f9134b6a9672bccd942522a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "03920e8109df487f9c770e8e76147d72": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57cb06d7781448aaa2235b8639d2dbdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "637541ffb379411eae93c1ea0a1a8c41": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "056e7a8e1df44ab6a4037e536be9e6c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc80fdb8240642c0b53c3ef6897aaf61": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63a99574876a46b69741e08f61ab4e95",
       "IPY_MODEL_33d759ba0bd34f4091b465ed83197ae5",
       "IPY_MODEL_287f2db4f4054f22b42decc3422eb18f"
      ],
      "layout": "IPY_MODEL_1bac1121bea44b9580c350328e7941f9"
     }
    },
    "63a99574876a46b69741e08f61ab4e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8519d6cd6904ded81a1f4bfa61b4ebe",
      "placeholder": "​",
      "style": "IPY_MODEL_c9bcbcbda05246b28d01b658255bbb08",
      "value": "100%"
     }
    },
    "33d759ba0bd34f4091b465ed83197ae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e7fb42b5de645c2b7524aaa83ec3988",
      "max": 25024,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1988de71d5d47c2b475701eae61f012",
      "value": 25024
     }
    },
    "287f2db4f4054f22b42decc3422eb18f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73f1f5bdbe9c4c35a74719bc88293175",
      "placeholder": "​",
      "style": "IPY_MODEL_02175537a2ae45db81bdb736a63283ca",
      "value": " 25024/25024 [00:02&lt;00:00, 8780.39it/s]"
     }
    },
    "1bac1121bea44b9580c350328e7941f9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8519d6cd6904ded81a1f4bfa61b4ebe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9bcbcbda05246b28d01b658255bbb08": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e7fb42b5de645c2b7524aaa83ec3988": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1988de71d5d47c2b475701eae61f012": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "73f1f5bdbe9c4c35a74719bc88293175": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02175537a2ae45db81bdb736a63283ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea2dadd59e3346c78294818c57c88953": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_162d6f247f904b9ab90a5bf231650af9",
       "IPY_MODEL_d288ab6801014e149c7de3273cd23b42",
       "IPY_MODEL_ebfbb1c3f5a8412e91d0784ff6026acb"
      ],
      "layout": "IPY_MODEL_10ec9e7387524bff94f84a34e9e5d47c"
     }
    },
    "162d6f247f904b9ab90a5bf231650af9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd1abc24809249fb8655a4bfbed20e87",
      "placeholder": "​",
      "style": "IPY_MODEL_dc4feca36f834df3ab5128f9abbeab91",
      "value": "100%"
     }
    },
    "d288ab6801014e149c7de3273cd23b42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c49a41709564a918a007d3944e98307",
      "max": 12326,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_816cbecd6e3449558aa09c68219f2b1e",
      "value": 12326
     }
    },
    "ebfbb1c3f5a8412e91d0784ff6026acb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10f75132c73748b4a6b87662708a7160",
      "placeholder": "​",
      "style": "IPY_MODEL_3d3334b0d5cb4b899e6eee184b1a4ae8",
      "value": " 12326/12326 [00:01&lt;00:00, 9234.74it/s]"
     }
    },
    "10ec9e7387524bff94f84a34e9e5d47c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd1abc24809249fb8655a4bfbed20e87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc4feca36f834df3ab5128f9abbeab91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c49a41709564a918a007d3944e98307": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "816cbecd6e3449558aa09c68219f2b1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "10f75132c73748b4a6b87662708a7160": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d3334b0d5cb4b899e6eee184b1a4ae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c83b705362e9464b85b98b46f00636c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_35f4c10ff879455d8e33a64db00c3a80",
       "IPY_MODEL_f42d684d747147928cfc4d0c137ffac0",
       "IPY_MODEL_1b5ffdecd026473d8a1cd3990dc2d1ef"
      ],
      "layout": "IPY_MODEL_ce259a9c2f1344a8859aef3425656b1f"
     }
    },
    "35f4c10ff879455d8e33a64db00c3a80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a59f66fe8c784aa9bd1508ad0502206a",
      "placeholder": "​",
      "style": "IPY_MODEL_3ca6130cc5b441c8b022e2b4cb098713",
      "value": "100%"
     }
    },
    "f42d684d747147928cfc4d0c137ffac0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bd8dcc715134724b5b552915a27e2bb",
      "max": 25024,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d5ec9377509547a7ae7a0257c4dd1060",
      "value": 25024
     }
    },
    "1b5ffdecd026473d8a1cd3990dc2d1ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd6a878ab0d54d0a95d0a29e0788ce88",
      "placeholder": "​",
      "style": "IPY_MODEL_3b0aedb01090432e919d1fce356c1a5c",
      "value": " 25024/25024 [00:01&lt;00:00, 13738.14it/s]"
     }
    },
    "ce259a9c2f1344a8859aef3425656b1f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a59f66fe8c784aa9bd1508ad0502206a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ca6130cc5b441c8b022e2b4cb098713": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7bd8dcc715134724b5b552915a27e2bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5ec9377509547a7ae7a0257c4dd1060": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bd6a878ab0d54d0a95d0a29e0788ce88": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b0aedb01090432e919d1fce356c1a5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88571b59fd40473480fe106eba5892ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0dcf88605b38411184ad38ec916b5937",
       "IPY_MODEL_0532fd5b9af24364806077e8558dec4d",
       "IPY_MODEL_4136931f09da413aa24bad84459b390d"
      ],
      "layout": "IPY_MODEL_8e19ddc5e25640fa95a9e975b003bbca"
     }
    },
    "0dcf88605b38411184ad38ec916b5937": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2f77f2a014c47898673126fb775fecb",
      "placeholder": "​",
      "style": "IPY_MODEL_4b308c3c9d3b44b6a284fe5f67c930f3",
      "value": "100%"
     }
    },
    "0532fd5b9af24364806077e8558dec4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be3ecf4e7e6643d193a96ecce0cca31c",
      "max": 12326,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e92e98d6aa524d15896e4a2338c57687",
      "value": 12326
     }
    },
    "4136931f09da413aa24bad84459b390d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d616caad734340bdb2a9722e59e593bd",
      "placeholder": "​",
      "style": "IPY_MODEL_68e59d5a9dad438fb101a05d4f2e66fd",
      "value": " 12326/12326 [00:00&lt;00:00, 13683.90it/s]"
     }
    },
    "8e19ddc5e25640fa95a9e975b003bbca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2f77f2a014c47898673126fb775fecb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b308c3c9d3b44b6a284fe5f67c930f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be3ecf4e7e6643d193a96ecce0cca31c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e92e98d6aa524d15896e4a2338c57687": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d616caad734340bdb2a9722e59e593bd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68e59d5a9dad438fb101a05d4f2e66fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
